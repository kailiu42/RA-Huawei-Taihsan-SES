<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>SUSE® Enterprise Storage on Huawei Taishan Implementation Guide</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" /><meta name="generator" content="DAPS 3.0.0 (https://opensuse.github.io/daps) using SUSE XSL Stylesheets 2.0.15 (based on DocBook XSL Stylesheets 1.79.2)" /><meta name="book-title" content="SUSE® Enterprise Storage on Huawei Taishan Implementation Guide" />

<script type="text/javascript">

var protocol = window.location.protocol.toLowerCase();
if ( protocol != 'file:' ) {
  var agent = navigator.userAgent.toLowerCase();
  var wanted = ( protocol == 'https:') ? 'https' : 'http';
  var file = 'fonts.css';
  document.write('<link rel="stylesheet" type="text/css" href="' + wanted + '://static.opensuse.org/fonts/'+ file +'"></link>');
}
else {
   document.write('<link rel="stylesheet" type="text/css" href="static/css/fonts-onlylocal.css"></link>');
}

</script><noscript><link rel="stylesheet" type="text/css" href="http://static.opensuse.org/fonts/fonts.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="single offline js-off"><div class="bypass-block"><a href="#_content">Jump to content</a></div><div id="_outer-wrap"><div id="_white-bg"><div id="_header"><div id="_logo"><img src="static/images/logo.png" alt="Logo" /></div><div class="crumbs inactive"><a class="single-crumb" href="#id-1" accesskey="c"><span class="single-contents-icon"></span>SUSE® Enterprise Storage on Huawei Taishan Implementation Guide</a><div class="bubble-corner active-contents"></div></div><div class="clearme"></div></div></div><div id="_fixed-header-wrap" class="inactive"><div id="_fixed-header"><div class="crumbs inactive"><a class="single-crumb" href="#id-1" accesskey="c"><span class="single-contents-icon"></span>Show Contents: SUSE® Enterprise Storage on Huawei Taishan Implementation Guide</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="clearme"></div></div><div class="clearme"></div></div><div class="active-contents bubble"><div class="bubble-container"><div id="_bubble-toc"><ol><li class="inactive"><a href="#_introduction"><span class="number">1 </span><span class="name">Introduction</span></a></li><li class="inactive"><a href="#_target_audience"><span class="number">2 </span><span class="name">Target Audience</span></a></li><li class="inactive"><a href="#_hardware_software"><span class="number">3 </span><span class="name">Hardware &amp; Software</span></a></li><li class="inactive"><a href="#_business_problem_and_business_value"><span class="number">4 </span><span class="name">Business Problem and Business Value</span></a></li><li class="inactive"><a href="#_requirements"><span class="number">5 </span><span class="name">Requirements</span></a></li><li class="inactive"><a href="#_architectural_overview"><span class="number">6 </span><span class="name">Architectural Overview</span></a></li><li class="inactive"><a href="#_component_model"><span class="number">7 </span><span class="name">Component Model</span></a></li><li class="inactive"><a href="#_deployment"><span class="number">8 </span><span class="name">Deployment</span></a></li><li class="inactive"><a href="#_conclusion"><span class="number">9 </span><span class="name">Conclusion</span></a></li><li class="inactive"><a href="#_references_and_resources"><span class="number">10 </span><span class="name">References and Resources</span></a></li><li class="inactive"><a href="#_bill_of_materials"><span class="number">A </span><span class="name">Bill of Materials</span></a></li><li class="inactive"><a href="#appendix-switch"><span class="number">B </span><span class="name">Network Switch Configuration</span></a></li><li class="inactive"><a href="#appendix-policy-cfg"><span class="number">C </span><span class="name">policy.cfg</span></a></li><li class="inactive"><a href="#_performance_data"><span class="number">D </span><span class="name">Performance Data</span></a></li></ol></div><div class="clearme"></div></div></div></div><div id="_toc-bubble-wrap"></div><div id="_content" class=""><div class="documentation"><div xml:lang="en" class="book" id="id-1" lang="en"><div class="titlepage"><div><div><h1 class="title">SUSE® Enterprise Storage on Huawei Taishan Implementation Guide</h1></div><div><span class="imprint-label">Author: </span><span class="firstname ">Kai</span> <span class="surname ">Liu</span></div><div class="date"><span class="imprint-label">Publication Date: </span>May 2 2019</div></div></div><div class="toc"><dl><dt><span class="chapter"><a href="#_introduction"><span class="number">1 </span><span class="name">Introduction</span></a></span></dt><dt><span class="chapter"><a href="#_target_audience"><span class="number">2 </span><span class="name">Target Audience</span></a></span></dt><dt><span class="chapter"><a href="#_hardware_software"><span class="number">3 </span><span class="name">Hardware &amp; Software</span></a></span></dt><dt><span class="chapter"><a href="#_business_problem_and_business_value"><span class="number">4 </span><span class="name">Business Problem and Business Value</span></a></span></dt><dd><dl><dt><span class="section"><a href="#_suse_enterprise_storage"><span class="number">4.1 </span><span class="name">SUSE Enterprise Storage</span></a></span></dt><dt><span class="section"><a href="#_huawei_taishan"><span class="number">4.2 </span><span class="name">Huawei Taishan</span></a></span></dt><dt><span class="section"><a href="#_business_problem"><span class="number">4.3 </span><span class="name">Business Problem</span></a></span></dt><dt><span class="section"><a href="#_business_value"><span class="number">4.4 </span><span class="name">Business Value</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#_requirements"><span class="number">5 </span><span class="name">Requirements</span></a></span></dt><dd><dl><dt><span class="section"><a href="#_functional_requirements"><span class="number">5.1 </span><span class="name">Functional Requirements</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#_architectural_overview"><span class="number">6 </span><span class="name">Architectural Overview</span></a></span></dt><dd><dl><dt><span class="section"><a href="#_solution_architecture"><span class="number">6.1 </span><span class="name">Solution Architecture</span></a></span></dt><dt><span class="section"><a href="#_networking_architecture"><span class="number">6.2 </span><span class="name">Networking Architecture</span></a></span></dt><dt><span class="section"><a href="#_networkip_address_scheme"><span class="number">6.3 </span><span class="name">Network/IP Address Scheme</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#_component_model"><span class="number">7 </span><span class="name">Component Model</span></a></span></dt><dt><span class="chapter"><a href="#_deployment"><span class="number">8 </span><span class="name">Deployment</span></a></span></dt><dd><dl><dt><span class="section"><a href="#_network_deployment_overview"><span class="number">8.1 </span><span class="name">Network Deployment Overview</span></a></span></dt><dt><span class="section"><a href="#_hardware_recommended_actions"><span class="number">8.2 </span><span class="name">Hardware Recommended Actions</span></a></span></dt><dt><span class="section"><a href="#_operating_system_installation"><span class="number">8.3 </span><span class="name">Operating System Installation</span></a></span></dt><dt><span class="section"><a href="#_suse_enterprise_storage_installation_configuration"><span class="number">8.4 </span><span class="name">SUSE Enterprise Storage Installation &amp; Configuration</span></a></span></dt><dt><span class="section"><a href="#_post_deployment_quick_tests"><span class="number">8.5 </span><span class="name">Post-deployment Quick Tests</span></a></span></dt><dt><span class="section"><a href="#_deployment_considerations"><span class="number">8.6 </span><span class="name">Deployment Considerations</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#_conclusion"><span class="number">9 </span><span class="name">Conclusion</span></a></span></dt><dt><span class="chapter"><a href="#_references_and_resources"><span class="number">10 </span><span class="name">References and Resources</span></a></span></dt><dt><span class="appendix"><a href="#_bill_of_materials"><span class="number">A </span><span class="name">Bill of Materials</span></a></span></dt><dt><span class="appendix"><a href="#appendix-switch"><span class="number">B </span><span class="name">Network Switch Configuration</span></a></span></dt><dt><span class="appendix"><a href="#appendix-policy-cfg"><span class="number">C </span><span class="name">policy.cfg</span></a></span></dt><dt><span class="appendix"><a href="#_performance_data"><span class="number">D </span><span class="name">Performance Data</span></a></span></dt><dd><dl><dt><span class="section"><a href="#_sequential_writes"><span class="number">D.1 </span><span class="name">Sequential Writes</span></a></span></dt><dt><span class="section"><a href="#_sequential_reads"><span class="number">D.2 </span><span class="name">Sequential Reads</span></a></span></dt><dt><span class="section"><a href="#_random_writes"><span class="number">D.3 </span><span class="name">Random Writes</span></a></span></dt><dt><span class="section"><a href="#_random_reads"><span class="number">D.4 </span><span class="name">Random Reads</span></a></span></dt><dt><span class="section"><a href="#_backuprecovery_simulations"><span class="number">D.5 </span><span class="name">Backup/Recovery Simulations</span></a></span></dt><dt><span class="section"><a href="#_kvm_virtual_guest_simulation"><span class="number">D.6 </span><span class="name">KVM Virtual Guest Simulation</span></a></span></dt><dt><span class="section"><a href="#_database_simulations"><span class="number">D.7 </span><span class="name">Database Simulations</span></a></span></dt></dl></dd></dl></div><div class="list-of-figures"><div class="toc-title">List of Figures</div><dl><dt><span class="figure"><a href="#img-ses-arch"><span class="number">6.1 </span><span class="name">SES Architecture</span></a></span></dt><dt><span class="figure"><a href="#img-ceph-network"><span class="number">6.2 </span><span class="name">Ceph Network Architecture</span></a></span></dt></dl></div><div class="list-of-tables"><div class="toc-title">List of Tables</div><dl><dt><span class="table"><a href="#id-1.7.5.3"><span class="number">6.1 </span><span class="name">Node Roles and Network Addresses</span></a></span></dt><dt><span class="table"><a href="#id-1.15.10.4"><span class="number">D.1 </span><span class="name">CephFS Sequential Writes</span></a></span></dt><dt><span class="table"><a href="#id-1.15.10.5"><span class="number">D.2 </span><span class="name">RBD Sequential Writes</span></a></span></dt><dt><span class="table"><a href="#id-1.15.11.3"><span class="number">D.3 </span><span class="name">CephFS Sequential Reads</span></a></span></dt><dt><span class="table"><a href="#id-1.15.11.4"><span class="number">D.4 </span><span class="name">RBD Sequential Reads</span></a></span></dt><dt><span class="table"><a href="#id-1.15.12.3"><span class="number">D.5 </span><span class="name">CephFS Random Writes</span></a></span></dt><dt><span class="table"><a href="#id-1.15.12.4"><span class="number">D.6 </span><span class="name">RBD Random Writes</span></a></span></dt><dt><span class="table"><a href="#id-1.15.13.3"><span class="number">D.7 </span><span class="name">CephFS Random Reads</span></a></span></dt><dt><span class="table"><a href="#id-1.15.13.4"><span class="number">D.8 </span><span class="name">RBD Random Reads</span></a></span></dt><dt><span class="table"><a href="#id-1.15.14.4"><span class="number">D.9 </span><span class="name">Backup Simulation</span></a></span></dt><dt><span class="table"><a href="#id-1.15.14.6"><span class="number">D.10 </span><span class="name">Recovery Simulation</span></a></span></dt><dt><span class="table"><a href="#id-1.15.15.3"><span class="number">D.11 </span><span class="name">VM Simulation</span></a></span></dt><dt><span class="table"><a href="#id-1.15.16.4"><span class="number">D.12 </span><span class="name">OLTP Log</span></a></span></dt><dt><span class="table"><a href="#id-1.15.16.6"><span class="number">D.13 </span><span class="name">OLTP Data</span></a></span></dt></dl></div><div class="chapter " id="_introduction"><div class="titlepage"><div><div><h1 class="title"><span class="number">1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Introduction</span> <a title="Permalink" class="permalink" href="#_introduction">#</a></h1></div></div></div><div class="line"></div><p>The objective of this guide is to present a step-by-step guide on how to implement SUSE Enterprise Storage v5.5 on the Huawei Taishan platform. It is suggested that the document be read in its entirety, along with the supplemental appendix information before attempting the process.</p><p>The deployment presented in this guide aligns with architectural best practices and will support the implementation of all currently supported protocols as identified in the SUSE Enterprise Storage documentation.</p><p>Upon completion of the steps in this document, a working SUSE Enterprise Storage v5.5 cluster will be operational as described in the <a class="link" href="https://www.suse.com/documentation/ses-5/book_storage_admin/data/book_storage_admin.html" target="_blank">SUSE Enterprise Storage Deployment and Administration Guide</a>.</p></div><div class="chapter " id="_target_audience"><div class="titlepage"><div><div><h1 class="title"><span class="number">2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Target Audience</span> <a title="Permalink" class="permalink" href="#_target_audience">#</a></h1></div></div></div><div class="line"></div><p>This reference architecture is targeted at administrators who deploy software defined storage solutions within their data centers and making the different storage services accessible to their own customer base. By following this document as well as those referenced herein, the administrator should have a full view of the SUSE Enterprise Storage architecture, deployment and administrative tasks, with a specific set of recommendations for deployment of the hardware and networking platform.</p></div><div class="chapter " id="_hardware_software"><div class="titlepage"><div><div><h1 class="title"><span class="number">3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Hardware &amp; Software</span> <a title="Permalink" class="permalink" href="#_hardware_software">#</a></h1></div></div></div><div class="line"></div><p>The recommended architecture for SUSE Enterprise Storage on Huawei Taishan leverages two models of Huawei servers. The role and functionality of each type of system within the SUSE Enterprise Storage environment will be explained in more detail in the architectural overview section.</p><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">Storage Nodes: </span><a title="Permalink" class="permalink" href="#id-1.4.3">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p><a class="link" href="https://www.suse.com/nbswebapp/yesBulletin.jsp?bulletinNumber=147070" target="_blank">Huawei 5280</a></p></li></ul></div><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">Admin, monitor, and protocol gateways: </span><a title="Permalink" class="permalink" href="#id-1.4.4">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p><a class="link" href="https://www.suse.com/nbswebapp/yesBulletin.jsp?bulletinNumber=146997" target="_blank">Huawei 2280</a></p></li></ul></div><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">Switches: </span><a title="Permalink" class="permalink" href="#id-1.4.5">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p>Huawei CE6851-48S6Q-HI 10Gb</p></li></ul></div><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">Software: </span><a title="Permalink" class="permalink" href="#id-1.4.6">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p>SUSE Enterprise Storage v5.5</p></li><li class="listitem "><p>SUSE Linux Enterprise Server 12 SP3</p></li></ul></div><div id="id-1.4.7" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>Please note that limited use subscriptions are provided with SUSE Enterprise Storage as part of the subscription entitlement</p></div></div><div class="chapter " id="_business_problem_and_business_value"><div class="titlepage"><div><div><h1 class="title"><span class="number">4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Business Problem and Business Value</span> <a title="Permalink" class="permalink" href="#_business_problem_and_business_value">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#_suse_enterprise_storage"><span class="number">4.1 </span><span class="name">SUSE Enterprise Storage</span></a></span></dt><dt><span class="section"><a href="#_huawei_taishan"><span class="number">4.2 </span><span class="name">Huawei Taishan</span></a></span></dt><dt><span class="section"><a href="#_business_problem"><span class="number">4.3 </span><span class="name">Business Problem</span></a></span></dt><dt><span class="section"><a href="#_business_value"><span class="number">4.4 </span><span class="name">Business Value</span></a></span></dt></dl></div></div><div class="sect1" id="_suse_enterprise_storage"><div class="titlepage"><div><div><h2 class="title" id="_suse_enterprise_storage"><span class="number">4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SUSE Enterprise Storage</span> <a title="Permalink" class="permalink" href="#_suse_enterprise_storage">#</a></h2></div></div></div><p>SUSE Enterprise Storage delivers a highly scalable, resilient, self-healing storage system designed for large scale environments ranging from hundreds of Terabytes to Petabytes. This software defined storage product can reduce IT costs by leveraging industry standard servers to present unified storage servicing block, file, and object protocols. Having storage that can meet the current needs and requirements of the data center while supporting topologies and protocols demanded by new web-scale applications, enables administrators to support the ever-increasing storage requirements of the enterprise with ease.</p></div><div class="sect1" id="_huawei_taishan"><div class="titlepage"><div><div><h2 class="title" id="_huawei_taishan"><span class="number">4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Huawei Taishan</span> <a title="Permalink" class="permalink" href="#_huawei_taishan">#</a></h2></div></div></div><p>Huawei Taishan servers provide a cost effective and scalable platform for the deployment of SUSE Enterprise Storage. These platforms unlocks the full potential of the Kunpeng CPU, raising the bar of SPECint benchmark by 25%, with up to 128 cores, 32 DDR4 DIMMs, PCIe 4.0, CCIX, and 100 GE LOM.</p><p>Taishan servers are powered by 64-bit ARMv8 Hi1616 processors, each with 32 cores of 2.4 GHz frequency. They support diverse interfaces such as PCIe 3.0, 10GE, and SAS/SATA, and integrate high performance with low power consumption.</p><p>Featuring models tailored for computing, storage, or balanced needs, Taishan is perfect for demanding workloads such as big data analytics, database acceleration, high-performance computing, cloud services, and native mobile applications. Taishan servers empower data centers with the ultimate efficiency.</p></div><div class="sect1" id="_business_problem"><div class="titlepage"><div><div><h2 class="title" id="_business_problem"><span class="number">4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Business Problem</span> <a title="Permalink" class="permalink" href="#_business_problem">#</a></h2></div></div></div><p>Customers of all sizes face a major storage challenge: While the overall cost per Terabyte of physical storage has gone down over the years, a data growth explosion took place driven by the need to access and leverage new data sources (ex: external sources such as social media) and the ability to "manage" new data types (ex: unstructured or object data). These ever increasing "data lakes" need different access methods: File, block, or object.</p><p>Addressing these challenges with legacy storage solutions would require either a number of specialized products (usually driven by access method) with traditional protection schemes (ex: RAID). These solutions struggle when scaling from Terabytes to Petabytes at reasonable cost and performance levels.</p></div><div class="sect1" id="_business_value"><div class="titlepage"><div><div><h2 class="title" id="_business_value"><span class="number">4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Business Value</span> <a title="Permalink" class="permalink" href="#_business_value">#</a></h2></div></div></div><p>This software defined storage solution enables transformation of the enterprise infrastructure by providing a unified platform where structured and unstructured data can co-exist and be accessed as file, block, or object depending on the application requirements. The combination of open-source software (Ceph) and industry standard servers reduce cost while providing the on-ramp to unlimited scalability needed to keep up with future demands.</p></div></div><div class="chapter " id="_requirements"><div class="titlepage"><div><div><h1 class="title"><span class="number">5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Requirements</span> <a title="Permalink" class="permalink" href="#_requirements">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#_functional_requirements"><span class="number">5.1 </span><span class="name">Functional Requirements</span></a></span></dt></dl></div></div><p>Enterprise storage systems require reliability, manageability, and serviceability. The legacy storage players have established a high threshold for each of these areas and now expect the software defined storage solutions to offer the same. Focusing on these areas helps SUSE make open source technology enterprise consumable. When combined with highly reliable and manageable hardware from Huawei, the result is a solution that meets the customer’s expectation.</p><div class="sect1" id="_functional_requirements"><div class="titlepage"><div><div><h2 class="title" id="_functional_requirements"><span class="number">5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Functional Requirements</span> <a title="Permalink" class="permalink" href="#_functional_requirements">#</a></h2></div></div></div><p>A SUSE Enterprise Storage solution is:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Simple to setup and deploy, within the documented guidelines of system hardware, networking and environmental prerequisites.</p></li><li class="listitem "><p>Adaptable to the physical and logical constraints needed by the business, both initially and as needed over time for performance, security, and scalability concerns.</p></li><li class="listitem "><p>Resilient to changes in physical infrastructure components, caused by failure or required maintenance.</p></li><li class="listitem "><p>Capable of providing optimized object and block services to client access nodes, either directly or through gateway services.</p></li></ul></div></div></div><div class="chapter " id="_architectural_overview"><div class="titlepage"><div><div><h1 class="title"><span class="number">6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Architectural Overview</span> <a title="Permalink" class="permalink" href="#_architectural_overview">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#_solution_architecture"><span class="number">6.1 </span><span class="name">Solution Architecture</span></a></span></dt><dt><span class="section"><a href="#_networking_architecture"><span class="number">6.2 </span><span class="name">Networking Architecture</span></a></span></dt><dt><span class="section"><a href="#_networkip_address_scheme"><span class="number">6.3 </span><span class="name">Network/IP Address Scheme</span></a></span></dt></dl></div></div><p>This architecture overview section complements the <a class="link" href="https://www.suse.com/docrep/documents/1mdg7eq2kz/suse_enterprise_storage_technical_overview_wp.pdf" target="_blank">SUSE Enterprise Storage Technical Overview</a> document available online which presents the concepts behind software defined storage and Ceph as well as a quick start guide (non-platform specific).</p><div class="sect1" id="_solution_architecture"><div class="titlepage"><div><div><h2 class="title" id="_solution_architecture"><span class="number">6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Solution Architecture</span> <a title="Permalink" class="permalink" href="#_solution_architecture">#</a></h2></div></div></div><p>SUSE Enterprise Storage provides unified block, file, and object access based on Ceph. Ceph is a distributed storage solution designed for scalability, reliability and performance. A critical component of Ceph is the RADOS object storage. RADOS enables a number of storage nodes to function together to store and retrieve data from the cluster using object storage techniques. The result is a storage solution that is abstracted from the hardware.</p><p>Ceph supports both native and traditional client access. The native clients are aware of the storage topology and communicate directly with the storage daemons over the public network, resulting in horizontally scaling performance. Non-native protocols, such as ISCSI, S3, and NFS require the use of gateways. While these gateways may be thought of as a limiting factor, the iSCSI and S3 gateways can scale horizontally using load balancing techniques.</p><div class="figure" id="img-ses-arch"><div class="figure-contents"><div class="mediaobject" align="center"><a xmlns="" href="images/SES-Reference-Architecture.png"><img src="images/SES-Reference-Architecture.png" width="" alt="SES Architecture" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 6.1: </span><span class="name">SES Architecture </span><a title="Permalink" class="permalink" href="#img-ses-arch">#</a></h6></div></div><p>In addition to the required network infrastructure, the minimum SUSE Enterprise Storage cluster comprises of a minimum of one administration server (physical or virtual), four object storage device nodes (OSDs), and three monitor nodes (MONs).</p><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">Specific to this implementation: </span><a title="Permalink" class="permalink" href="#id-1.7.3.6">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p>One system is deployed as the administrative host server. The administration host is the Salt master and hosts the SUSE Enterprise Storage Administration Interface, openATTIC, which is the central management system which supports the cluster.</p></li><li class="listitem "><p>Three systems are deployed as monitor (MONs) nodes. Monitor nodes maintain information about the cluster health state, a map of the other monitor nodes and a CRUSH map. They also keep history of changes performed to the cluster.</p></li><li class="listitem "><p>It is strongly recommended to deploy monitors and other services on dedicated nodes. However due to shortage of equipment the monitors were deployed on the OSD nodes in this specific reference setup.</p></li><li class="listitem "><p>Additional servers may be deployed as iSCSI gateway nodes. iSCSI is a storage area network (SAN) protocol that allows clients (called initiators) to send SCSI command to SCSI storage devices (targets) on remote servers. This protocol is utilized for block-based connectivity to environments such as Microsoft Windows, VMware, and traditional UNIX. These systems may be scaled horizontally through client usage of multi-path technology.</p></li><li class="listitem "><p>The RADOS gateway provides S3 and Swift based access methods to the cluster. These nodes are generally situated behind a load balancer infrastructure to provide redundancy and scalability. It is important to note that the load generated by the RADOS gateway can consume a significant amount of compute and memory resources making the minimum recommended configuration contain 6-8 CPU cores and 32GB of RAM.</p></li><li class="listitem "><p>SUSE Enterprise Storage requires a minimum of four systems as storage nodes. The storage nodes contain individual storage devices that are each assigned an Object Storage Daemon (OSD). The OSD daemon assigned to the device stores data and manages the data replication and rebalancing processes. OSD daemons also communicate with the monitor (MON) nodes and provide them with the state of the other OSD daemons.</p></li></ul></div></div><div class="sect1" id="_networking_architecture"><div class="titlepage"><div><div><h2 class="title" id="_networking_architecture"><span class="number">6.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networking Architecture</span> <a title="Permalink" class="permalink" href="#_networking_architecture">#</a></h2></div></div></div><p>A software-defined solution is only as reliable as its slowest and least redundant component. This makes it important to design and implement a robust, high performance storage network infrastructure. From a network perspective for Ceph, this translates into:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Separation of cluster (backend) and client-facing (public)network traffic. This isolates Ceph OSD daemon replication activities from Ceph clients. This may be achieved through separate physical networks or through use of VLANs.</p></li><li class="listitem "><p>Redundancy and capacity in the form of bonded network interfaces connected to switches.</p></li></ul></div><p><a class="xref" href="#img-ceph-network" title="Ceph Network Architecture">Figure 6.2, “Ceph Network Architecture”</a> shows the logical layout of the traditional Ceph cluster implementation.</p><div class="figure" id="img-ceph-network"><div class="figure-contents"><div class="mediaobject" align="center"><a xmlns="" href="images/Ceph-Network.png"><img src="images/Ceph-Network.png" width="" alt="Ceph Network Architecture" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 6.2: </span><span class="name">Ceph Network Architecture </span><a title="Permalink" class="permalink" href="#img-ceph-network">#</a></h6></div></div></div><div class="sect1" id="_networkip_address_scheme"><div class="titlepage"><div><div><h2 class="title" id="_networkip_address_scheme"><span class="number">6.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network/IP Address Scheme</span> <a title="Permalink" class="permalink" href="#_networkip_address_scheme">#</a></h2></div></div></div><p>Specific to this implementation, the following naming and addressing scheme were utilized.</p><div class="table" id="id-1.7.5.3"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 6.1: </span><span class="name">Node Roles and Network Addresses </span><a title="Permalink" class="permalink" href="#id-1.7.5.3">#</a></h6></div><div class="table-contents"><table class="table" summary="Node Roles and Network Addresses" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /><col class="col_4" /></colgroup><thead><tr><th align="left" valign="top">Role</th><th align="left" valign="top">Hostname</th><th align="left" valign="top">Public Network</th><th align="left" valign="top">Cluster Network</th></tr></thead><tbody><tr><td align="left" valign="top"><p>Admin</p></td><td align="left" valign="top"><p>sesadmin.example.com</p></td><td align="left" valign="top"><p>10.1.1.3</p></td><td align="left" valign="top"><p>N/A</p></td></tr><tr><td align="left" valign="top"><p>Monitor</p></td><td align="left" valign="top"><p>osd1.example.com</p></td><td align="left" valign="top"><p>10.1.1.4</p></td><td align="left" valign="top"><p>N/A</p></td></tr><tr><td align="left" valign="top"><p>Monitor</p></td><td align="left" valign="top"><p>osd2.example.com</p></td><td align="left" valign="top"><p>10.1.1.5</p></td><td align="left" valign="top"><p>N/A</p></td></tr><tr><td align="left" valign="top"><p>Monitor</p></td><td align="left" valign="top"><p>osd3.example.com</p></td><td align="left" valign="top"><p>10.1.1.6</p></td><td align="left" valign="top"><p>N/A</p></td></tr><tr><td align="left" valign="top"><p>OSD Node</p></td><td align="left" valign="top"><p>osd1.example.com</p></td><td align="left" valign="top"><p>10.1.1.4</p></td><td align="left" valign="top"><p>10.2.1.4</p></td></tr><tr><td align="left" valign="top"><p>OSD Node</p></td><td align="left" valign="top"><p>osd2.example.com</p></td><td align="left" valign="top"><p>10.1.1.5</p></td><td align="left" valign="top"><p>10.2.1.5</p></td></tr><tr><td align="left" valign="top"><p>OSD Node</p></td><td align="left" valign="top"><p>osd3.example.com</p></td><td align="left" valign="top"><p>10.1.1.6</p></td><td align="left" valign="top"><p>10.2.1.6</p></td></tr><tr><td align="left" valign="top"><p>OSD Node</p></td><td align="left" valign="top"><p>osd4.example.com</p></td><td align="left" valign="top"><p>10.1.1.7</p></td><td align="left" valign="top"><p>10.2.1.7</p></td></tr></tbody></table></div></div></div></div><div class="chapter " id="_component_model"><div class="titlepage"><div><div><h1 class="title"><span class="number">7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Component Model</span> <a title="Permalink" class="permalink" href="#_component_model">#</a></h1></div></div></div><div class="line"></div><p>The preceding sections provided information on the both the overall Huawei hardware as well as an introduction to the Ceph software architecture. In this section, the focus is on the SUSE components: SUSE Linux Enterprise Server (SLES), SUSE Enterprise Storage (SES), and the Subscription Management Tool (SMT).</p><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">Component overview (SUSE) </span><a title="Permalink" class="permalink" href="#id-1.8.3">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p>SUSE Linux Enterprise Server - A world class secure, open source server operating system, equally adept at powering physical, virtual, or cloud-based mission-critical workloads. Service Pack 3 further raises the bar in helping organizations to accelerate innovation, enhance system reliability, meet tough security requirements and adapt to new technologies.</p></li><li class="listitem "><p>Subscription Management Tool for SLES - Allows enterprise customers to optimize the management of SUSE Linux Enterprise (and product such as SUSE Enterprise Storage) software updates and subscription entitlements. It establishes a proxy system for SUSE Customer Center with repository and registration targets.</p></li><li class="listitem "><p>SUSE Enterprise Storage - Provided as an product on top of SUSE Linux Enterprise Server, this intelligent software-defined storage solution, powered by Ceph technology with enterprise engineering and support from SUSE enables customers to transform enterprise infrastructure to reduce costs while providing unlimited scalability.</p></li></ul></div></div><div class="chapter " id="_deployment"><div class="titlepage"><div><div><h1 class="title"><span class="number">8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deployment</span> <a title="Permalink" class="permalink" href="#_deployment">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#_network_deployment_overview"><span class="number">8.1 </span><span class="name">Network Deployment Overview</span></a></span></dt><dt><span class="section"><a href="#_hardware_recommended_actions"><span class="number">8.2 </span><span class="name">Hardware Recommended Actions</span></a></span></dt><dt><span class="section"><a href="#_operating_system_installation"><span class="number">8.3 </span><span class="name">Operating System Installation</span></a></span></dt><dt><span class="section"><a href="#_suse_enterprise_storage_installation_configuration"><span class="number">8.4 </span><span class="name">SUSE Enterprise Storage Installation &amp; Configuration</span></a></span></dt><dt><span class="section"><a href="#_post_deployment_quick_tests"><span class="number">8.5 </span><span class="name">Post-deployment Quick Tests</span></a></span></dt><dt><span class="section"><a href="#_deployment_considerations"><span class="number">8.6 </span><span class="name">Deployment Considerations</span></a></span></dt></dl></div></div><p>This deployment section should be seen as a supplement online <a class="link" href="https://www.suse.com/documentation/" target="_blank">documentation</a>. Specifically, the <a class="link" href="https://www.suse.com/documentation/suse-enterprise-storage-5/book_storage_deployment/data/book_storage_deployment.html" target="_blank">SUSE Enterprise Storage 5 Deployment Guide</a> as well as <a class="link" href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/book_sle_admin.html" target="_blank">SUSE Linux Enterprise Server Administration Guide</a>. It is assumed that a Subscription Management Tool server exists within the environment. If not, please follow the information in <a class="link" href="https://www.suse.com/documentation/sles-12/book_smt/data/book_smt.html" target="_blank">Subscription Management Tool (SMT) for SLES</a> to make one available. The emphasis is on specific design and configuration choices.</p><p>In this document we use <code class="literal">example.com</code> as the domain name for the nodes, replace it with your real domain name in your own installation.</p><div class="sect1" id="_network_deployment_overview"><div class="titlepage"><div><div><h2 class="title" id="_network_deployment_overview"><span class="number">8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Deployment Overview</span> <a title="Permalink" class="permalink" href="#_network_deployment_overview">#</a></h2></div></div></div><p>The following considerations for the network configuration should be attended to:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Ensure that all network switches are updated with consistent firmware versions.</p></li><li class="listitem "><p>Configure 802.3ad for system port bonding between the switches, plus enable jumbo frames.</p></li><li class="listitem "><p>Specific configuration for this deployment can be found in <a class="xref" href="#appendix-switch" title="Appendix B. Network Switch Configuration">Appendix B, <em>Network Switch Configuration</em></a></p></li><li class="listitem "><p>Network IP addressing and IP ranges need proper planning. In optimal environments, a dedicated storage subnet should be used for all SUSE Enterprise Storage nodes on the primary network, with a separate, dedicated subnet for the cluster network. Depending on the size of the installation, ranges larger than /24 may be required. When planning the network, current as well as future growth should be taken into consideration.</p></li><li class="listitem "><p>Setup DNS A records for all nodes. Decide on subnets and VLANs and configure the switch ports accordingly.</p></li><li class="listitem "><p>Always use the same type (short or full) of hostname every where, for Salt master/minions etc.</p></li><li class="listitem "><p>Ensure that you have access to a valid, reliable NTP service, as this is a critical requirement for all nodes. If not, it is recommended to use the admin node.</p></li></ul></div></div><div class="sect1" id="_hardware_recommended_actions"><div class="titlepage"><div><div><h2 class="title" id="_hardware_recommended_actions"><span class="number">8.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Hardware Recommended Actions</span> <a title="Permalink" class="permalink" href="#_hardware_recommended_actions">#</a></h2></div></div></div><p>The following considerations for the hardware platforms should be attended to:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Configure the boot media as RAID-1.</p></li><li class="listitem "><p>Configure all data and journal devices as individual RAID-0 if RAID controllers with hardware write cache are configured.</p></li></ul></div></div><div class="sect1" id="_operating_system_installation"><div class="titlepage"><div><div><h2 class="title" id="_operating_system_installation"><span class="number">8.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Operating System Installation</span> <a title="Permalink" class="permalink" href="#_operating_system_installation">#</a></h2></div></div></div><p>There are several key tasks to ensure are performed correctly during the operating system installation. During the SUSE Linux Enterprise installation, be sure and register the system with an update server. Ideally, this is a local SMT server which will reduce the time required for updates to be downloaded and applied to all nodes. By updating the nodes during installation, the system will deploy with the most up-to-date packages available, helping to ensure the best experience possible.</p><p>To speed installation, on the System Role screen, it is suggested to select KVM Virtualization Host. When the Installation Settings screen is reached, select <span class="strong"><strong>Software</strong></span> and then un-check KVM Host Server. The resulting installation is a text mode server that is an appropriate base OS for SUSE Enterprise Server.</p><p>The next item is to ensure that the operating system is installed on the correct device. Especially on OSD nodes, the system may not choose the right drive by default. The proper way to ensure the right device is being used is to select <span class="strong"><strong>Create Partition Setup</strong></span> on the Suggested Partitioning screen. This will then display a list of devices, allowing selection of the correct boot device. Next select <span class="strong"><strong>Edit Proposal Settings</strong></span> and unselect the <span class="strong"><strong>Propose Separate Home Partition</strong></span> checkbox.</p><p>Do ensure that NTP is configured to point to a valid, physical NTP server. This is critical for SUSE Enterprise Storage to function properly, and failure to do so can result in an unhealthy or non-functional cluster. And keep in mind that the NTP service is not designed to be run on an virtualized environment, so make sure the NTP server been used is an physical machine or it may cause strange clock drifting problem.</p></div><div class="sect1" id="_suse_enterprise_storage_installation_configuration"><div class="titlepage"><div><div><h2 class="title" id="_suse_enterprise_storage_installation_configuration"><span class="number">8.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SUSE Enterprise Storage Installation &amp; Configuration</span> <a title="Permalink" class="permalink" href="#_suse_enterprise_storage_installation_configuration">#</a></h2></div></div></div><div class="sect2" id="_software_deployment_configuration_deepsea_and_salt"><div class="titlepage"><div><div><h3 class="title" id="_software_deployment_configuration_deepsea_and_salt"><span class="number">8.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Software Deployment Configuration (Deepsea and Salt)</span> <a title="Permalink" class="permalink" href="#_software_deployment_configuration_deepsea_and_salt">#</a></h3></div></div></div><p>Salt, along with DeepSea, is a stack of components that help deploy and manage server infrastructure. It is very scalable, fast, and relatively easy to get running.</p><p>There are three key Salt imperatives that need to be followed:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>The Salt Master is the host that controls the entire cluster deployment. Ceph itself should NOT be running on the master as all resources should be dedicated to Salt master services. In our scenario, we used the Admin host as the Salt master.</p></li><li class="listitem "><p>Salt minions are nodes controlled by Salt master. OSD, monitor, and gateway nodes are all Salt minions in this installation.</p></li><li class="listitem "><p>Salt minions need to correctly resolve the Salt master’s host name over the network. This can be achieved through configuring unique host names per interface (eg osd1-cluster.example.com and osd1-public.example.com) in DNS and/or local /etc/hosts files.</p></li></ul></div><p>Deepsea consists of series of Salt files to automate the deployment and management of a Ceph cluster. It consolidates the administrator’s decision making in a single location around cluster assignment, role assignment and profile assignment. Deepsea collects each set of tasks into a goal or stage.</p><p>The following steps, performed in order, will be used for this reference implementation:</p></div><div class="sect2" id="_prepare_all_nodes"><div class="titlepage"><div><div><h3 class="title" id="_prepare_all_nodes"><span class="number">8.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prepare All Nodes</span> <a title="Permalink" class="permalink" href="#_prepare_all_nodes">#</a></h3></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Install DeepSea on the Salt master which is the Admin node:</p><div class="verbatim-wrap"><pre class="screen">zypper in salt-master</pre></div></li><li class="listitem "><p>Start the salt-master service and enable:</p><div class="verbatim-wrap"><pre class="screen">systemctl enable --now salt-master.service</pre></div></li><li class="listitem "><p>Install the salt-minion on all cluster nodes (including the Admin):</p><div class="verbatim-wrap"><pre class="screen">zypper in salt-minion</pre></div></li><li class="listitem "><p>Configure all minions to connect to the Salt master:</p><p>Modify the entry for master in the <span class="emphasis"><em>/etc/salt/minion</em></span></p><div class="verbatim-wrap"><pre class="screen">master: sesadmin.example.com</pre></div></li><li class="listitem "><p>Restart the salt-minion service and enable it:</p><div class="verbatim-wrap"><pre class="screen">systemctl restart salt-minion.service
systemctl enable salt-minion.service</pre></div></li><li class="listitem "><p>List Salt fingerprints on all the minions:</p><div class="verbatim-wrap"><pre class="screen">salt-call --local key.finger</pre></div></li><li class="listitem "><p>List all minion fingerprints on the Salt master, verify them against the fingerprints on each minions to make sure they all match. If they do, accept all Salt keys on the Salt master:</p><div class="verbatim-wrap"><pre class="screen">salt-key -F
salt-key --list-all
salt-key –-accept-all</pre></div></li><li class="listitem "><p>Verify if Salt works properly by "ping" each minions. They should all return True on success:</p><div class="verbatim-wrap"><pre class="screen">salt '*' test.ping</pre></div></li></ol></div></div><div class="sect2" id="_prepare_osd_disks_on_osd_nodes"><div class="titlepage"><div><div><h3 class="title" id="_prepare_osd_disks_on_osd_nodes"><span class="number">8.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prepare OSD Disks on OSD Nodes</span> <a title="Permalink" class="permalink" href="#_prepare_osd_disks_on_osd_nodes">#</a></h3></div></div></div><p>If the OSD nodes were used in a prior installation, zap ALL the OSD disks first.</p><div id="id-1.9.7.4.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>This must be done on all the OSD disks that were used in a prior installation, or else the deployment will fail when activating OSDs.</p></div><div id="id-1.9.7.4.4" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>Below commands should not be copied and executed on your installation blindly. The device names used below are just examples, you need to change them to match only the OSD disks in your own installation. Failed to use the correct device name may erase your OS disk or other disks that may hold valuable data.</p></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Wipe the beginning of each partition:</p><div class="verbatim-wrap"><pre class="screen">for partition in /dev/sdX[0-9]*
do
  dd if=/dev/zero of=$partition bs=4096 count=1 oflag=direct
done</pre></div></li><li class="listitem "><p>Wipe the partition table:</p><div class="verbatim-wrap"><pre class="screen">sgdisk -Z --clear -g /dev/sdX</pre></div></li><li class="listitem "><p>Wipe the backup partition tables:</p><div class="verbatim-wrap"><pre class="screen">size=`blockdev --getsz /dev/sdX`
position=$((size/4096 - 33))
dd if=/dev/zero of=/dev/sdX bs=4M count=33 seek=$position oflag=direct</pre></div></li></ol></div></div><div class="sect2" id="_install_and_configure_deepsea_on_admin_node"><div class="titlepage"><div><div><h3 class="title" id="_install_and_configure_deepsea_on_admin_node"><span class="number">8.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Install and Configure Deepsea on Admin Node</span> <a title="Permalink" class="permalink" href="#_install_and_configure_deepsea_on_admin_node">#</a></h3></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Install deepsea package on Admin node:</p><div class="verbatim-wrap"><pre class="screen"># zypper in deepsea</pre></div></li><li class="listitem "><p>Check <span class="emphasis"><em>/srv/pillar/ceph/master_minion.sls</em></span> for correctness.</p></li><li class="listitem "><p>Check <span class="emphasis"><em>/srv/pillar/ceph/deepsea_minions.sls</em></span> file, make sure the deepsea_minions option targets the correct nodes. In the usual case, it can simply be put like below to match all Salt minions in the cluster:</p><div class="verbatim-wrap"><pre class="screen">deepsea_minions: '*'</pre></div></li><li class="listitem "><p>Create <span class="emphasis"><em>/srv/pillar/ceph/stack/ceph/cluster.yml</em></span> <span id="create-cluster-yml"></span> with below options:</p><div class="verbatim-wrap"><pre class="screen">cluster_network: &lt;net/mask of cluster network&gt;
public_network: &lt;net/mask of public network&gt;
time_server: &lt;Address of NTP server, if this line is omitted admin node will be used&gt;</pre></div></li></ol></div></div><div class="sect2" id="_deploy_using_deepsea"><div class="titlepage"><div><div><h3 class="title" id="_deploy_using_deepsea"><span class="number">8.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploy Using Deepsea</span> <a title="Permalink" class="permalink" href="#_deploy_using_deepsea">#</a></h3></div></div></div><p>At this point Deepsea commands can be run to deploy the cluster.</p><div id="id-1.9.7.6.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>Each command can be run either as:</p><div class="verbatim-wrap"><pre class="screen">salt-run state.orch ceph.stage.&lt;stage name&gt;</pre></div><p>Or:</p><div class="verbatim-wrap"><pre class="screen">deepsea stage run ceph.stage.&lt;stage name&gt;</pre></div><p>The latter form is preferred as it outputs real time progress.</p></div><div class="sect3" id="_stage_0_prepare"><div class="titlepage"><div><div><h4 class="title" id="_stage_0_prepare"><span class="number">8.4.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Stage 0: Prepare</span> <a title="Permalink" class="permalink" href="#_stage_0_prepare">#</a></h4></div></div></div><p>During this stage, all required updates are applied and your system may be rebooted.</p><div class="verbatim-wrap"><pre class="screen">deepsea stage run ceph.stage.0</pre></div><div id="id-1.9.7.6.4.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>If the Salt master reboots during Stage 0, you need to run Stage 0 again after it boots up.</p></div><p>Optionally, create the /var/lib/ceph btrfs subvolume:</p><div class="verbatim-wrap"><pre class="screen">salt-run state.orch ceph.migrate.subvolume</pre></div></div><div class="sect3" id="_stage_1_discovery"><div class="titlepage"><div><div><h4 class="title" id="_stage_1_discovery"><span class="number">8.4.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Stage 1: Discovery</span> <a title="Permalink" class="permalink" href="#_stage_1_discovery">#</a></h4></div></div></div><p>During this stage, all hardware in your cluster is detected and necessary information are collected for the Ceph configuration.</p><div class="verbatim-wrap"><pre class="screen">deepsea stage run ceph.stage.1</pre></div><div id="id-1.9.7.6.5.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Configure cluster and public network in <span class="emphasis"><em>/srv/pillar/ceph/stack/ceph/cluster.yml</em></span> if not yet done as described in <a class="xref" href="#create-cluster-yml">Create cluster.yml</a>.</p></div><p>Now a <span class="emphasis"><em>/srv/pillar/ceph/proposals/policy.cfg</em></span> file needs to be created to instruct Deepsea on the location and configuration files to use for the different components that make up the Ceph cluster (Salt master, admin, monitor, OSD and other roles).</p><p>To do so, copy the example file to the right location then edit it to match your installation:</p><div class="verbatim-wrap"><pre class="screen">cp /usr/share/doc/packages/deepsea/examples/policy.cfg-rolebased /srv/pillar/ceph/proposals/policy.cfg</pre></div><div id="id-1.9.7.6.5.8" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>See <a class="xref" href="#appendix-policy-cfg" title="Appendix C. policy.cfg">Appendix C, <em>policy.cfg</em></a> for the one used when installing the cluster described in this document.</p></div><p>A proposal for the storage layout needs to be generated at this time. For the hardware configuration used for this work, the following command was utilized:</p><div class="verbatim-wrap"><pre class="screen">salt-run proposal.populate</pre></div><p>The proposal generator will automatically use hard disks for OSD storage and NVMe SSDs for BlueStore WAL and DB storage.</p><div id="id-1.9.7.6.5.12" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>On your own deployment you may need to play with the proposal generator with different arguments for several times to get what you really want.</p><p>To print the help text about the various arguments proposal command accepts:</p><div class="verbatim-wrap"><pre class="screen">salt-run proposal.help</pre></div><p>To show the generated proposal on screen according to the arguments passed:</p><div class="verbatim-wrap"><pre class="screen">salt-run proposal.peek &lt;arguments&gt;</pre></div><p>To write the proposal to the <span class="emphasis"><em>/srv/pillar/ceph/proposals</em></span> subdirectory:</p><div class="verbatim-wrap"><pre class="screen">salt-run proposal.populate &lt;arguments&gt; name=myprofile</pre></div><p>Pass the argument <code class="literal">name=myprofile</code> to the command to name the storage profile. This will result in a <code class="literal">profile-myprofile</code> subdirectory been created to store the new proposal files.</p></div></div><div class="sect3" id="_stage_2_configure"><div class="titlepage"><div><div><h4 class="title" id="_stage_2_configure"><span class="number">8.4.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Stage 2: Configure</span> <a title="Permalink" class="permalink" href="#_stage_2_configure">#</a></h4></div></div></div><p>During this stage necessary configuration data are prepared in particular format.</p><div class="verbatim-wrap"><pre class="screen">deepsea stage run ceph.stage.2</pre></div><div id="id-1.9.7.6.6.4" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>Use below command to check the attributes of each node:</p><div class="verbatim-wrap"><pre class="screen">salt '*' pillar.items</pre></div></div></div><div class="sect3" id="_stage_3_deploy"><div class="titlepage"><div><div><h4 class="title" id="_stage_3_deploy"><span class="number">8.4.5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Stage 3: Deploy</span> <a title="Permalink" class="permalink" href="#_stage_3_deploy">#</a></h4></div></div></div><p>A basic Ceph cluster with mandatory Ceph services is created.</p><div class="verbatim-wrap"><pre class="screen">deepsea stage run ceph.stage.3</pre></div><div id="id-1.9.7.6.7.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>It may take quite some time for above command to finish if your cluster is large, or your Internet bandwidth is limited while you didn’t register the nodes to local SMT server.</p></div><p>After the above command is finished successfully, check whether the cluster is up by running:</p><div class="verbatim-wrap"><pre class="screen">ceph -s</pre></div></div><div class="sect3" id="_stage_4_services"><div class="titlepage"><div><div><h4 class="title" id="_stage_4_services"><span class="number">8.4.5.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Stage 4: Services</span> <a title="Permalink" class="permalink" href="#_stage_4_services">#</a></h4></div></div></div><p>Additional features of Ceph like iSCSI, Object Gateway and CephFS can be installed in this stage. Each is optional and up to your situation.</p><div class="verbatim-wrap"><pre class="screen">deepsea stage run ceph.stage.4</pre></div></div></div></div><div class="sect1" id="_post_deployment_quick_tests"><div class="titlepage"><div><div><h2 class="title" id="_post_deployment_quick_tests"><span class="number">8.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Post-deployment Quick Tests</span> <a title="Permalink" class="permalink" href="#_post_deployment_quick_tests">#</a></h2></div></div></div><p>The steps below can be used (regardless of the deployment method) to validate the overall cluster health:</p><div class="verbatim-wrap"><pre class="screen">ceph status
ceph osd pool create test 1024
rados bench –p test 300 write --no-cleanup
rados bench –p test 300 seq</pre></div><p>Once the tests are complete, you can remove the test pool via:</p><div class="verbatim-wrap"><pre class="screen">ceph tell mon.* injectargs --mon-allow-pool-delete=true
ceph osd pool delete test test --yes-i-really-really-mean-it
ceph tell mon.* injectargs --mon-allow-pool-delete=false</pre></div></div><div class="sect1" id="_deployment_considerations"><div class="titlepage"><div><div><h2 class="title" id="_deployment_considerations"><span class="number">8.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deployment Considerations</span> <a title="Permalink" class="permalink" href="#_deployment_considerations">#</a></h2></div></div></div><p>Some final considerations before deploying your own version of a SUSE Enterprise Storage cluster, based on Ceph. As previously stated, please refer to the Administration and Deployment Guide.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>With the default replication setting of 3, remember that the client-facing network will have about half or less of the traffic of the backend network. This is especially true when component failures occur or rebalancing happens on the OSD nodes. For this reason, it is important not to under provision this critical cluster and service resource.</p></li><li class="listitem "><p>It is important to maintain the minimum number of monitor nodes at three. As the cluster increases in size, it is best to increment in pairs, keeping the total number of Mon nodes as an odd number. However, only very large or very distributed clusters would likely need beyond the 3 MON nodes cited in this reference implementation. For performance reasons, it is recommended to use distinct nodes for the MON roles, so that the OSD nodes can be scaled as capacity requirements dictate.</p></li><li class="listitem "><p>Although in this specific implementation monitors were deployed on the OSD nodes due to shortage of equipment, ideally monitors should be deployed on dedicated nodes.</p></li><li class="listitem "><p>As described in this implementation guide and the SUSE Enterprise Storage documentation, a minimum of four OSD nodes is recommended, with the default replication setting of 3. This will ensure cluster operation, even with the loss of a complete OSD node. Generally speaking, performance of the overall cluster increases as more properly configured OSD nodes are added.</p></li></ul></div></div></div><div class="chapter " id="_conclusion"><div class="titlepage"><div><div><h1 class="title"><span class="number">9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Conclusion</span> <a title="Permalink" class="permalink" href="#_conclusion">#</a></h1></div></div></div><div class="line"></div><p>The Huawei Taishan series represents a strong capacity-oriented platform. When combined with the access flexibility and reliability of SUSE Enterprise Storage and the industry leading support from Huawei, any business can feel confident in the ability to address the exponential growth in storage they are currently faced with.</p></div><div class="chapter " id="_references_and_resources"><div class="titlepage"><div><div><h1 class="title"><span class="number">10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">References and Resources</span> <a title="Permalink" class="permalink" href="#_references_and_resources">#</a></h1></div></div></div><div class="line"></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.11.2.1"><span class="term ">SUSE Enterprise Storage Technical Overview</span></dt><dd><p><a class="link" href="https://www.suse.com/media/white-paper/suse_enterprise_storage_technical_overview_wp.pdf" target="_blank">https://www.suse.com/media/white-paper/suse_enterprise_storage_technical_overview_wp.pdf</a></p></dd><dt id="id-1.11.2.2"><span class="term ">SUSE Enterprise Storage Tech Specs</span></dt><dd><p><a class="link" href="https://www.suse.com/products/suse-enterprise-storage/#tech-specs" target="_blank">https://www.suse.com/products/suse-enterprise-storage/#tech-specs</a></p></dd><dt id="id-1.11.2.3"><span class="term ">SUSE Enterprise Storage - Deployment Guide</span></dt><dd><p><a class="link" href="https://www.suse.com/documentation/suse-enterprise-storage-5/singlehtml/book_storage_deployment/book_storage_deployment.html" target="_blank">https://www.suse.com/documentation/suse-enterprise-storage-5/singlehtml/book_storage_deployment/book_storage_deployment.html</a></p></dd><dt id="id-1.11.2.4"><span class="term ">SUSE Enterprise Storage - Administration Guide</span></dt><dd><p><a class="link" href="https://www.suse.com/documentation/suse-enterprise-storage-5/singlehtml/book_storage_admin/book_storage_admin.html" target="_blank">https://www.suse.com/documentation/suse-enterprise-storage-5/singlehtml/book_storage_admin/book_storage_admin.html</a></p></dd><dt id="id-1.11.2.5"><span class="term ">SUSE Linux Enterprise Server 12 SP3 - Deployment Guide</span></dt><dd><p><a class="link" href="https://www.suse.com/documentation/sles-12/singlehtml/book_sle_deployment/book_sle_deployment.html" target="_blank">https://www.suse.com/documentation/sles-12/singlehtml/book_sle_deployment/book_sle_deployment.html</a></p></dd><dt id="id-1.11.2.6"><span class="term ">SUSE Linux Enterprise Server 12 SP3 - Administration Guide</span></dt><dd><p><a class="link" href="https://www.suse.com/documentation/sles-12/singlehtml/book_sle_admin/book_sle_admin.html" target="_blank">https://www.suse.com/documentation/sles-12/singlehtml/book_sle_admin/book_sle_admin.html</a></p></dd><dt id="id-1.11.2.7"><span class="term ">SUSE Linux Enterprise Server 12 SP3 - Storage Administration Guide</span></dt><dd><p><a class="link" href="https://www.suse.com/documentation/sles-12/singlehtml/stor_admin/stor_admin.html" target="_blank">https://www.suse.com/documentation/sles-12/singlehtml/stor_admin/stor_admin.html</a></p></dd><dt id="id-1.11.2.8"><span class="term ">Subscription Management Tool for SLES 12 SP3</span></dt><dd><p><a class="link" href="https://www.suse.com/documentation/sles-12/singlehtml/book_smt/book_smt.html" target="_blank">https://www.suse.com/documentation/sles-12/singlehtml/book_smt/book_smt.html</a></p></dd></dl></div></div><div class="appendix " id="_bill_of_materials"><div class="titlepage"><div><div><h1 class="title"><span class="number">A </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Bill of Materials</span> <a title="Permalink" class="permalink" href="#_bill_of_materials">#</a></h1></div></div></div><div class="line"></div><div class="informaltable"><table class="informaltable" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /><col class="col_4" /></colgroup><thead><tr><th align="left" valign="top">Role</th><th align="left" valign="top">Qty</th><th align="left" valign="top">Component</th><th align="left" valign="top">Notes</th></tr></thead><tbody><tr><td align="left" valign="top"><p>Admin Node</p></td><td align="left" valign="top"><p>1</p></td><td align="left" valign="top"><p>A VM on a Huawei x86 machine</p></td><td align="left" valign="top"><p>The node consists of:</p>
<div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>8x vCPU cores</p></li><li class="listitem "><p>64GB RAM</p></li><li class="listitem "><p>100GB virtual disk for OS</p></li><li class="listitem "><p>2x virtual NIC port to public and cluster network</p></li></ul></div></td></tr><tr><td align="left" valign="top"><p>OSD nodes. MON, MGR shared the OSD hosts</p></td><td align="left" valign="top"><p>4</p></td><td align="left" valign="top"><p>Huawei Taishan 5280</p></td><td align="left" valign="top"><p>Each node consists of:</p>
<div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>2x HiSilicon 1616</p></li><li class="listitem "><p>256GB</p></li><li class="listitem "><p>1x HiSilicon 1616 Integrated SAS Controller</p></li><li class="listitem "><p>2x 300GB SAS HDD for OS</p></li><li class="listitem "><p>34x 4TB 7.2k SATA HDD for OSD</p></li><li class="listitem "><p>2x Huawei 1TB NVMe SSD for db and journal</p></li><li class="listitem "><p>1x HiSilicon 1616 Embedded Network Controller - 1GbE Quad Port</p></li><li class="listitem "><p>1x Dual Port Intel 82599 10Gb Ethernet adapter</p></li></ul></div></td></tr><tr><td align="left" valign="top"><p>Software</p></td><td align="left" valign="top"><p>1</p></td><td align="left" valign="top"><p>SUSE Enterprise Storage v5.5 Subscription Base configuration</p></td><td align="left" valign="top"><p>Allows for 4 storage nodes and 6 infrastructure nodes. Expansion subscriptions need to be purchased if more nodes need to be added to the cluster.</p></td></tr><tr><td align="left" valign="top"><p>Switches</p></td><td align="left" valign="top"><p>2</p></td><td align="left" valign="top"><p>Huawei CE6851-48S6Q-HI</p></td><td align="left" valign="top"><p>32 Ports of 100GbE</p></td></tr></tbody></table></div></div><div class="appendix " id="appendix-switch"><div class="titlepage"><div><div><h1 class="title"><span class="number">B </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Switch Configuration</span> <a title="Permalink" class="permalink" href="#appendix-switch">#</a></h1></div></div></div><div class="line"></div><p>Below are the commands used to configure the Huawei CE6851-48S6Q-HI switch for 802.3ad link aggregation.</p><div class="verbatim-wrap"><pre class="screen">port link-type trunk
mode lacp-static
load-balance src-dst-ip
local-preference disable</pre></div><p>In the OS bonding configuration, make sure below options are set:</p><div class="verbatim-wrap"><pre class="screen">BONDING_MODULE_OPTS='mode=802.3ad miimon=100 lacp_rate=fast xmit_hash_policy=layer3+4'</pre></div></div><div class="appendix " id="appendix-policy-cfg"><div class="titlepage"><div><div><h1 class="title"><span class="number">C </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">policy.cfg</span> <a title="Permalink" class="permalink" href="#appendix-policy-cfg">#</a></h1></div></div></div><div class="line"></div><div class="verbatim-wrap highlight yaml"><pre class="screen">## Cluster Assignment
cluster-ceph/cluster/*.sls

## Roles
# ADMIN
role-master/cluster/sesadmin*.sls
role-admin/cluster/sesadmin*.sls

# MON
role-mon/cluster/osd[1-3].sls

# MGR (mgrs are usually colocated with mons)
role-mgr/cluster/osd[1-3]*.sls

# MDS
role-mds/cluster/osd1*.sls

# openATTIC
role-openattic/cluster/sesadmin*.sls

# COMMON
config/stack/default/global.yml
config/stack/default/ceph/cluster.yml

## Profiles
profile-default/cluster/*.sls
profile-default/stack/default/ceph/minions/*.yml</pre></div></div><div class="appendix " id="_performance_data"><div class="titlepage"><div><div><h1 class="title"><span class="number">D </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Performance Data</span> <a title="Permalink" class="permalink" href="#_performance_data">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#_sequential_writes"><span class="number">D.1 </span><span class="name">Sequential Writes</span></a></span></dt><dt><span class="section"><a href="#_sequential_reads"><span class="number">D.2 </span><span class="name">Sequential Reads</span></a></span></dt><dt><span class="section"><a href="#_random_writes"><span class="number">D.3 </span><span class="name">Random Writes</span></a></span></dt><dt><span class="section"><a href="#_random_reads"><span class="number">D.4 </span><span class="name">Random Reads</span></a></span></dt><dt><span class="section"><a href="#_backuprecovery_simulations"><span class="number">D.5 </span><span class="name">Backup/Recovery Simulations</span></a></span></dt><dt><span class="section"><a href="#_kvm_virtual_guest_simulation"><span class="number">D.6 </span><span class="name">KVM Virtual Guest Simulation</span></a></span></dt><dt><span class="section"><a href="#_database_simulations"><span class="number">D.7 </span><span class="name">Database Simulations</span></a></span></dt></dl></div></div><p>Comprehensive performance baselines are run as part of a reference build activity. This activity yields a vast amount of information that may be used to approximate the size and performance of the solution. The only tuning applied is documented in the implementation portion of this document.</p><p>The tests are comprised of a number of Flexible I/O (fio) job files run against multiple worker nodes. The job files and testing scripts may be found for review at: <a class="link" href="https://github.com/dmbyte/benchmaster" target="_blank">https://github.com/dmbyte/benchmaster</a>. This is a personal repository and no warranties are made in regard to the fitness and safety of the scripts found there.</p><p>The testing methodology involves two different types of long running tests. The types and duration of the tests have very specific purposes. There are both I/O simulation jobs and single metric jobs.</p><p>The length of the test run, in combination with the ramp-up time specified in the job file, is intended to allow the system to overrun caches. This is a worst-case scenario for a system and would indicate that it is running at or near capacity. Given that few applications can tolerate significant amounts of long tail latencies, the job files have specific latency targets assigned. These targets are intended to be in-line with expectations for the type of I/O operation being performed and set realistic expectations for the application environment.</p><p>The latency target, when combined with the latency window and latency window percentage set the minimum number of I/Os that must be within the latency target during the latency window time period. For most of the tests, the latency target is 20ms of less. The latency window is five seconds and the latency target is 99.99999%. The way that fio uses this is to ramp up the queue depth at each new latency window boundary until more than .00001% of all I/O’s during a five second window are higher than 20ms. At that point, fio backs the queue depth down where the latency target is sustainable.</p><p>In the figures below, the x-axis labels indicate the block size in KiB on the top line and the data protection scheme on the bottom line. 3xRep is indicative of the Ceph standard 3 replica configuration for data protection while EC2+2 is Erasure Coded using the ISA plugin with k=2 and m=2. The Erasure Coding settings were selected to fit within the minimum cluster hardware size supported by SUSE.</p><p>These settings, along with block size, max queue depth, jobs per node, etc., are all visible in the job files found at the repository link above.</p><p>Load testing was provided by for additional Huawei x86 servers on the same 10GbE network</p><div class="sect1" id="_sequential_writes"><div class="titlepage"><div><div><h2 class="title" id="_sequential_writes"><span class="number">D.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Sequential Writes</span> <a title="Permalink" class="permalink" href="#_sequential_writes">#</a></h2></div></div></div><p>Sequential write I/O testing was performed across block sizes ranging from 4KiB to 4MiB.</p><p>These tests have latency targets associated. 4K is 10ms, 64K is 20ms, 1MiB is 100ms, and 4MiB is 300ms.</p><div class="table" id="id-1.15.10.4"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table D.1: </span><span class="name">CephFS Sequential Writes </span><a title="Permalink" class="permalink" href="#id-1.15.10.4">#</a></h6></div><div class="table-contents"><table class="table" summary="CephFS Sequential Writes" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /><col class="col_4" /><col class="col_5" /></colgroup><thead><tr><th align="left" valign="top">Data Protection</th><th align="left" valign="top">I/O Size</th><th align="left" valign="top">Write BW (MiB/s)</th><th align="left" valign="top">Write IOPS</th><th align="left" valign="top">Write Avg Latency (ms)</th></tr></thead><tbody><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>4K</p></td><td align="left" valign="top"><p>26</p></td><td align="left" valign="top"><p>6896.3</p></td><td align="left" valign="top"><p>9.2</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>4K</p></td><td align="left" valign="top"><p>6</p></td><td align="left" valign="top"><p>1759.6</p></td><td align="left" valign="top"><p>36.5</p></td></tr><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>64K</p></td><td align="left" valign="top"><p>99</p></td><td align="left" valign="top"><p>1587.9</p></td><td align="left" valign="top"><p>41.0</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>64K</p></td><td align="left" valign="top"><p>79</p></td><td align="left" valign="top"><p>1268.9</p></td><td align="left" valign="top"><p>50.5</p></td></tr><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>1M</p></td><td align="left" valign="top"><p>624</p></td><td align="left" valign="top"><p>624.3</p></td><td align="left" valign="top"><p>103.4</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>1M</p></td><td align="left" valign="top"><p>454</p></td><td align="left" valign="top"><p>454.8</p></td><td align="left" valign="top"><p>139.7</p></td></tr><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>4M</p></td><td align="left" valign="top"><p>977</p></td><td align="left" valign="top"><p>244.3</p></td><td align="left" valign="top"><p>262.2</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>4M</p></td><td align="left" valign="top"><p>826</p></td><td align="left" valign="top"><p>206.6</p></td><td align="left" valign="top"><p>306.0</p></td></tr></tbody></table></div></div><div class="table" id="id-1.15.10.5"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table D.2: </span><span class="name">RBD Sequential Writes </span><a title="Permalink" class="permalink" href="#id-1.15.10.5">#</a></h6></div><div class="table-contents"><table class="table" summary="RBD Sequential Writes" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /><col class="col_4" /><col class="col_5" /></colgroup><thead><tr><th align="left" valign="top">Data Protection</th><th align="left" valign="top">I/O Size</th><th align="left" valign="top">Write BW (MiB/s)</th><th align="left" valign="top">Write IOPS</th><th align="left" valign="top">Write Avg Latency (ms)</th></tr></thead><tbody><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>4K</p></td><td align="left" valign="top"><p>34</p></td><td align="left" valign="top"><p>8720.2</p></td><td align="left" valign="top"><p>7.2</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>4K</p></td><td align="left" valign="top"><p>5</p></td><td align="left" valign="top"><p>1299.3</p></td><td align="left" valign="top"><p>51.4</p></td></tr><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>64K</p></td><td align="left" valign="top"><p>143</p></td><td align="left" valign="top"><p>2297.6</p></td><td align="left" valign="top"><p>27.8</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>64K</p></td><td align="left" valign="top"><p>65</p></td><td align="left" valign="top"><p>1049.3</p></td><td align="left" valign="top"><p>60.4</p></td></tr><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>1M</p></td><td align="left" valign="top"><p>228</p></td><td align="left" valign="top"><p>228.0</p></td><td align="left" valign="top"><p>69.9</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>1M</p></td><td align="left" valign="top"><p>121</p></td><td align="left" valign="top"><p>121.4</p></td><td align="left" valign="top"><p>131.5</p></td></tr><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>4M</p></td><td align="left" valign="top"><p>1076</p></td><td align="left" valign="top"><p>269.1</p></td><td align="left" valign="top"><p>233.0</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>4M</p></td><td align="left" valign="top"><p>732</p></td><td align="left" valign="top"><p>183.2</p></td><td align="left" valign="top"><p>344.9</p></td></tr></tbody></table></div></div></div><div class="sect1" id="_sequential_reads"><div class="titlepage"><div><div><h2 class="title" id="_sequential_reads"><span class="number">D.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Sequential Reads</span> <a title="Permalink" class="permalink" href="#_sequential_reads">#</a></h2></div></div></div><p>The sequential read tests were conducted across the same range of block sizes as the write testing. The latency targets are only present for 4k sequential reads, where it is set to 10ms.</p><div class="table" id="id-1.15.11.3"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table D.3: </span><span class="name">CephFS Sequential Reads </span><a title="Permalink" class="permalink" href="#id-1.15.11.3">#</a></h6></div><div class="table-contents"><table class="table" summary="CephFS Sequential Reads" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /><col class="col_4" /><col class="col_5" /></colgroup><thead><tr><th align="left" valign="top">Data Protection</th><th align="left" valign="top">I/O Size</th><th align="left" valign="top">Read BW (MiB/s)</th><th align="left" valign="top">Read IOPS</th><th align="left" valign="top">Read Avg Latency (ms)</th></tr></thead><tbody><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>4K</p></td><td align="left" valign="top"><p>212</p></td><td align="left" valign="top"><p>54480.6</p></td><td align="left" valign="top"><p>1.2</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>4K</p></td><td align="left" valign="top"><p>31</p></td><td align="left" valign="top"><p>8077.4</p></td><td align="left" valign="top"><p>2.0</p></td></tr><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>64K</p></td><td align="left" valign="top"><p>2369</p></td><td align="left" valign="top"><p>37912.0</p></td><td align="left" valign="top"><p>54.2</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>64K</p></td><td align="left" valign="top"><p>2077</p></td><td align="left" valign="top"><p>33234.7</p></td><td align="left" valign="top"><p>42.5</p></td></tr><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>1M</p></td><td align="left" valign="top"><p>1740</p></td><td align="left" valign="top"><p>1739.4</p></td><td align="left" valign="top"><p>1079.4</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>1M</p></td><td align="left" valign="top"><p>1483</p></td><td align="left" valign="top"><p>1482.3</p></td><td align="left" valign="top"><p>1357.2</p></td></tr><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>4M</p></td><td align="left" valign="top"><p>2475</p></td><td align="left" valign="top"><p>617.9</p></td><td align="left" valign="top"><p>2897.8</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>4M</p></td><td align="left" valign="top"><p>3124</p></td><td align="left" valign="top"><p>780.2</p></td><td align="left" valign="top"><p>2560.1</p></td></tr></tbody></table></div></div><div class="table" id="id-1.15.11.4"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table D.4: </span><span class="name">RBD Sequential Reads </span><a title="Permalink" class="permalink" href="#id-1.15.11.4">#</a></h6></div><div class="table-contents"><table class="table" summary="RBD Sequential Reads" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /><col class="col_4" /><col class="col_5" /></colgroup><thead><tr><th align="left" valign="top">Data Protection</th><th align="left" valign="top">I/O Size</th><th align="left" valign="top">Read BW (MiB/s)</th><th align="left" valign="top">Read IOPS</th><th align="left" valign="top">Read Avg Latency (ms)</th></tr></thead><tbody><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>4K</p></td><td align="left" valign="top"><p>45</p></td><td align="left" valign="top"><p>11548.5</p></td><td align="left" valign="top"><p>1.4</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>4K</p></td><td align="left" valign="top"><p>80</p></td><td align="left" valign="top"><p>20667.4</p></td><td align="left" valign="top"><p>1.5</p></td></tr><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>64K</p></td><td align="left" valign="top"><p>1661</p></td><td align="left" valign="top"><p>26582.4</p></td><td align="left" valign="top"><p>19.3</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>64K</p></td><td align="left" valign="top"><p>1894</p></td><td align="left" valign="top"><p>30307.0</p></td><td align="left" valign="top"><p>61.8</p></td></tr><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>1M</p></td><td align="left" valign="top"><p>3595</p></td><td align="left" valign="top"><p>3654.1</p></td><td align="left" valign="top"><p>526.6</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>1M</p></td><td align="left" valign="top"><p>611</p></td><td align="left" valign="top"><p>611.5</p></td><td align="left" valign="top"><p>801.9</p></td></tr><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>4M</p></td><td align="left" valign="top"><p>4384</p></td><td align="left" valign="top"><p>1095.1</p></td><td align="left" valign="top"><p>1634.2</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>4M</p></td><td align="left" valign="top"><p>7887</p></td><td align="left" valign="top"><p>1970.8</p></td><td align="left" valign="top"><p>2120.1</p></td></tr></tbody></table></div></div></div><div class="sect1" id="_random_writes"><div class="titlepage"><div><div><h2 class="title" id="_random_writes"><span class="number">D.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Random Writes</span> <a title="Permalink" class="permalink" href="#_random_writes">#</a></h2></div></div></div><p>Random write tests were performed with the smaller I/O sizes of 4k and 64k. The 4k tests have a latency target of 10ms and the 64k tests have a latency target of 20ms.</p><div class="table" id="id-1.15.12.3"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table D.5: </span><span class="name">CephFS Random Writes </span><a title="Permalink" class="permalink" href="#id-1.15.12.3">#</a></h6></div><div class="table-contents"><table class="table" summary="CephFS Random Writes" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /><col class="col_4" /><col class="col_5" /></colgroup><thead><tr><th align="left" valign="top">Data Protection</th><th align="left" valign="top">I/O Size</th><th align="left" valign="top">Write BW (MiB/s)</th><th align="left" valign="top">Write IOPS</th><th align="left" valign="top">Write Avg Latency (ms)</th></tr></thead><tbody><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>4</p></td><td align="left" valign="top"><p>14</p></td><td align="left" valign="top"><p>3635.6</p></td><td align="left" valign="top"><p>17.6</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>4</p></td><td align="left" valign="top"><p>4</p></td><td align="left" valign="top"><p>1107.3</p></td><td align="left" valign="top"><p>57.8</p></td></tr><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>64</p></td><td align="left" valign="top"><p>104</p></td><td align="left" valign="top"><p>1666.9</p></td><td align="left" valign="top"><p>38.0</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>64</p></td><td align="left" valign="top"><p>59</p></td><td align="left" valign="top"><p>958.5</p></td><td align="left" valign="top"><p>66.8</p></td></tr></tbody></table></div></div><div class="table" id="id-1.15.12.4"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table D.6: </span><span class="name">RBD Random Writes </span><a title="Permalink" class="permalink" href="#id-1.15.12.4">#</a></h6></div><div class="table-contents"><table class="table" summary="RBD Random Writes" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /><col class="col_4" /><col class="col_5" /></colgroup><thead><tr><th align="left" valign="top">Data Protection</th><th align="left" valign="top">I/O Size</th><th align="left" valign="top">Write BW (MiB/s)</th><th align="left" valign="top">Write IOPS</th><th align="left" valign="top">Write Avg Latency (ms)</th></tr></thead><tbody><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>4</p></td><td align="left" valign="top"><p>10</p></td><td align="left" valign="top"><p>2606.4</p></td><td align="left" valign="top"><p>24.7</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>4</p></td><td align="left" valign="top"><p>3</p></td><td align="left" valign="top"><p>952.7</p></td><td align="left" valign="top"><p>67.0</p></td></tr><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>64</p></td><td align="left" valign="top"><p>142</p></td><td align="left" valign="top"><p>2286.6</p></td><td align="left" valign="top"><p>27.8</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>64</p></td><td align="left" valign="top"><p>50</p></td><td align="left" valign="top"><p>802.8</p></td><td align="left" valign="top"><p>79.7</p></td></tr></tbody></table></div></div></div><div class="sect1" id="_random_reads"><div class="titlepage"><div><div><h2 class="title" id="_random_reads"><span class="number">D.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Random Reads</span> <a title="Permalink" class="permalink" href="#_random_reads">#</a></h2></div></div></div><p>The random read tests were conducted on both 4K and 64K I/O sizes with latency targets of 10ms and 20ms respectively.</p><div class="table" id="id-1.15.13.3"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table D.7: </span><span class="name">CephFS Random Reads </span><a title="Permalink" class="permalink" href="#id-1.15.13.3">#</a></h6></div><div class="table-contents"><table class="table" summary="CephFS Random Reads" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /><col class="col_4" /><col class="col_5" /></colgroup><thead><tr><th align="left" valign="top">Data Protection</th><th align="left" valign="top">I/O Size</th><th align="left" valign="top">Read BW (MiB/s)</th><th align="left" valign="top">Read IOPS</th><th align="left" valign="top">Read Avg Latency (ms)</th></tr></thead><tbody><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>4K</p></td><td align="left" valign="top"><p>20</p></td><td align="left" valign="top"><p>5235.5</p></td><td align="left" valign="top"><p>12.2</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>4K</p></td><td align="left" valign="top"><p>9</p></td><td align="left" valign="top"><p>2543.5</p></td><td align="left" valign="top"><p>25.1</p></td></tr><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>64K</p></td><td align="left" valign="top"><p>282</p></td><td align="left" valign="top"><p>4522.5</p></td><td align="left" valign="top"><p>14.1</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>64K</p></td><td align="left" valign="top"><p>131</p></td><td align="left" valign="top"><p>2111.1</p></td><td align="left" valign="top"><p>30.3</p></td></tr></tbody></table></div></div><div class="table" id="id-1.15.13.4"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table D.8: </span><span class="name">RBD Random Reads </span><a title="Permalink" class="permalink" href="#id-1.15.13.4">#</a></h6></div><div class="table-contents"><table class="table" summary="RBD Random Reads" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /><col class="col_4" /><col class="col_5" /></colgroup><thead><tr><th align="left" valign="top">Data Protection</th><th align="left" valign="top">I/O Size</th><th align="left" valign="top">Read BW (MiB/s)</th><th align="left" valign="top">Read IOPS</th><th align="left" valign="top">Read Avg Latency (ms)</th></tr></thead><tbody><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>4K</p></td><td align="left" valign="top"><p>6</p></td><td align="left" valign="top"><p>1719.7</p></td><td align="left" valign="top"><p>9.3</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>4K</p></td><td align="left" valign="top"><p>19</p></td><td align="left" valign="top"><p>5007.9</p></td><td align="left" valign="top"><p>12.9</p></td></tr><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>64K</p></td><td align="left" valign="top"><p>75</p></td><td align="left" valign="top"><p>1206.7</p></td><td align="left" valign="top"><p>13.3</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>64K</p></td><td align="left" valign="top"><p>239</p></td><td align="left" valign="top"><p>3828.7</p></td><td align="left" valign="top"><p>16.7</p></td></tr></tbody></table></div></div></div><div class="sect1" id="_backuprecovery_simulations"><div class="titlepage"><div><div><h2 class="title" id="_backuprecovery_simulations"><span class="number">D.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Backup/Recovery Simulations</span> <a title="Permalink" class="permalink" href="#_backuprecovery_simulations">#</a></h2></div></div></div><p>The following test results are workload oriented.</p><p><span class="formalpara-title">Backup. </span>The backup simulation test attempts to simulate the SUSE Enterprise Storage cluster being used as a disk-based backup target that is either hosting file systems on RBDs or is using CephFS. The test had a latency target of 200ms at the time of the test run. The latency target has since been removed.</p><div class="table" id="id-1.15.14.4"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table D.9: </span><span class="name">Backup Simulation </span><a title="Permalink" class="permalink" href="#id-1.15.14.4">#</a></h6></div><div class="table-contents"><table class="table" summary="Backup Simulation" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /><col class="col_4" /><col class="col_5" /><col class="col_6" /></colgroup><thead><tr><th align="left" valign="top">Data Protection</th><th align="left" valign="top">Protocol</th><th align="left" valign="top">IO Size</th><th align="left" valign="top">Write BW (MiB/s)</th><th align="left" valign="top">Write IOPS</th><th align="left" valign="top">Write Avg Latency (ms)</th></tr></thead><tbody><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>CephFS</p></td><td align="left" valign="top"><p>64M</p></td><td align="left" valign="top"><p>881</p></td><td align="left" valign="top"><p>12.7</p></td><td align="left" valign="top"><p>160976.1</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>CephFS</p></td><td align="left" valign="top"><p>64M</p></td><td align="left" valign="top"><p>982</p></td><td align="left" valign="top"><p>14.3</p></td><td align="left" valign="top"><p>138143.5</p></td></tr><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>RBD</p></td><td align="left" valign="top"><p>64M</p></td><td align="left" valign="top"><p>1038</p></td><td align="left" valign="top"><p>15.2</p></td><td align="left" valign="top"><p>167619.1</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>RBD</p></td><td align="left" valign="top"><p>64M</p></td><td align="left" valign="top"><p>887</p></td><td align="left" valign="top"><p>12.8</p></td><td align="left" valign="top"><p>182757.4</p></td></tr></tbody></table></div></div><p><span class="formalpara-title">Recovery. </span>The recovery workload is intended to simulate recovery jobs being run from SUSE Enterprise Storage. It tests both RBD and CephFS.</p><div class="table" id="id-1.15.14.6"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table D.10: </span><span class="name">Recovery Simulation </span><a title="Permalink" class="permalink" href="#id-1.15.14.6">#</a></h6></div><div class="table-contents"><table class="table" summary="Recovery Simulation" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /><col class="col_4" /><col class="col_5" /><col class="col_6" /></colgroup><thead><tr><th align="left" valign="top">Data Protection</th><th align="left" valign="top">Protocol</th><th align="left" valign="top">IO Size</th><th align="left" valign="top">Read BW (MiB/s)</th><th align="left" valign="top">Read IOPS</th><th align="left" valign="top">Read Avg Latency (ms)</th></tr></thead><tbody><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>CephFS</p></td><td align="left" valign="top"><p>64M</p></td><td align="left" valign="top"><p>2494</p></td><td align="left" valign="top"><p>37.9</p></td><td align="left" valign="top"><p>52540.2</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>CephFS</p></td><td align="left" valign="top"><p>64M</p></td><td align="left" valign="top"><p>2770</p></td><td align="left" valign="top"><p>42.2</p></td><td align="left" valign="top"><p>54579.5</p></td></tr><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>RBD</p></td><td align="left" valign="top"><p>64M</p></td><td align="left" valign="top"><p>1327</p></td><td align="left" valign="top"><p>20.5</p></td><td align="left" valign="top"><p>23828.8</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>RBD</p></td><td align="left" valign="top"><p>64M</p></td><td align="left" valign="top"><p>2330</p></td><td align="left" valign="top"><p>35.5</p></td><td align="left" valign="top"><p>58883.9</p></td></tr></tbody></table></div></div></div><div class="sect1" id="_kvm_virtual_guest_simulation"><div class="titlepage"><div><div><h2 class="title" id="_kvm_virtual_guest_simulation"><span class="number">D.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">KVM Virtual Guest Simulation</span> <a title="Permalink" class="permalink" href="#_kvm_virtual_guest_simulation">#</a></h2></div></div></div><p>The kvm-krbd test roughly simulates virtual machines running. This test has a 20ms latency target and is 80% read with both reads and writes being random.</p><div class="table" id="id-1.15.15.3"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table D.11: </span><span class="name">VM Simulation </span><a title="Permalink" class="permalink" href="#id-1.15.15.3">#</a></h6></div><div class="table-contents"><table class="table" summary="VM Simulation" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /><col class="col_4" /><col class="col_5" /><col class="col_6" /><col class="col_7" /><col class="col_8" /><col class="col_9" /></colgroup><thead><tr><th align="left" valign="top">Data Protection</th><th align="left" valign="top">Protocol</th><th align="left" valign="top">IO Size</th><th align="left" valign="top">Write BW (MiB/s)</th><th align="left" valign="top">Write IOPS</th><th align="left" valign="top">Write Avg Latency (ms)</th><th align="left" valign="top">Read BW (MiB/s)</th><th align="left" valign="top">Read IOPS</th><th align="left" valign="top">Read Avg Latency (ms)</th></tr></thead><tbody><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>CephFS</p></td><td align="left" valign="top"><p>4K</p></td><td align="left" valign="top"><p>3</p></td><td align="left" valign="top"><p>960.2</p></td><td align="left" valign="top"><p>14.4</p></td><td align="left" valign="top"><p>15</p></td><td align="left" valign="top"><p>3856.5</p></td><td align="left" valign="top"><p>13.1</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>CephFS</p></td><td align="left" valign="top"><p>4K</p></td><td align="left" valign="top"><p>1</p></td><td align="left" valign="top"><p>264.9</p></td><td align="left" valign="top"><p>70.7</p></td><td align="left" valign="top"><p>4</p></td><td align="left" valign="top"><p>1084.0</p></td><td align="left" valign="top"><p>42.2</p></td></tr><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>RBD</p></td><td align="left" valign="top"><p>4K</p></td><td align="left" valign="top"><p>1</p></td><td align="left" valign="top"><p>271.9</p></td><td align="left" valign="top"><p>11.0</p></td><td align="left" valign="top"><p>4</p></td><td align="left" valign="top"><p>1099.9</p></td><td align="left" valign="top"><p>11.8</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>RBD</p></td><td align="left" valign="top"><p>4K</p></td><td align="left" valign="top"><p>1</p></td><td align="left" valign="top"><p>325.1</p></td><td align="left" valign="top"><p>67.1</p></td><td align="left" valign="top"><p>5</p></td><td align="left" valign="top"><p>1307.9</p></td><td align="left" valign="top"><p>28.6</p></td></tr></tbody></table></div></div></div><div class="sect1" id="_database_simulations"><div class="titlepage"><div><div><h2 class="title" id="_database_simulations"><span class="number">D.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Database Simulations</span> <a title="Permalink" class="permalink" href="#_database_simulations">#</a></h2></div></div></div><p>It is important to keep sight of the fact that Ceph is not designed for high performance database activity. These tests provide a baseline understanding of performance expectations should a database be deployed using SUSE Enterprise Storage.</p><p><span class="formalpara-title">OLTP Database Log. </span>The database log simulation is based on documented I/O patterns from several major database vendors. The I/O profile is 80% sequential 8KB writes with a latency target of 1ms.</p><div class="table" id="id-1.15.16.4"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table D.12: </span><span class="name">OLTP Log </span><a title="Permalink" class="permalink" href="#id-1.15.16.4">#</a></h6></div><div class="table-contents"><table class="table" summary="OLTP Log" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /><col class="col_4" /><col class="col_5" /><col class="col_6" /><col class="col_7" /><col class="col_8" /><col class="col_9" /></colgroup><thead><tr><th align="left" valign="top">Data Protection</th><th align="left" valign="top">Protocol</th><th align="left" valign="top">IO Size</th><th align="left" valign="top">Write BW (MiB/s)</th><th align="left" valign="top">Write IOPS</th><th align="left" valign="top">Write Avg Latency (ms)</th><th align="left" valign="top">Read BW (MiB/s)</th><th align="left" valign="top">Read IOPS</th><th align="left" valign="top">Read Avg Latency (ms)</th></tr></thead><tbody><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>CephFS</p></td><td align="left" valign="top"><p>8K</p></td><td align="left" valign="top"><p>33</p></td><td align="left" valign="top"><p>4312.5</p></td><td align="left" valign="top"><p>15.2</p></td><td align="left" valign="top"><p>8</p></td><td align="left" valign="top"><p>1077.8</p></td><td align="left" valign="top"><p>2.3</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>CephFS</p></td><td align="left" valign="top"><p>8K</p></td><td align="left" valign="top"><p>9</p></td><td align="left" valign="top"><p>1239.4</p></td><td align="left" valign="top"><p>45.9</p></td><td align="left" valign="top"><p>2</p></td><td align="left" valign="top"><p>306.8</p></td><td align="left" valign="top"><p>19.2</p></td></tr><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>RBD</p></td><td align="left" valign="top"><p>8K</p></td><td align="left" valign="top"><p>12</p></td><td align="left" valign="top"><p>1616.0</p></td><td align="left" valign="top"><p>9.6</p></td><td align="left" valign="top"><p>3</p></td><td align="left" valign="top"><p>406.3</p></td><td align="left" valign="top"><p>1.1</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>RBD</p></td><td align="left" valign="top"><p>8K</p></td><td align="left" valign="top"><p>8</p></td><td align="left" valign="top"><p>1083.8</p></td><td align="left" valign="top"><p>51.7</p></td><td align="left" valign="top"><p>2</p></td><td align="left" valign="top"><p>270.5</p></td><td align="left" valign="top"><p>19.7</p></td></tr></tbody></table></div></div><p><span class="formalpara-title">OLTP Database Datafile. </span>The OLTP Datafile simulation is set for an 80/20 mix of random reads and writes. The latency target is 10ms.</p><div class="table" id="id-1.15.16.6"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table D.13: </span><span class="name">OLTP Data </span><a title="Permalink" class="permalink" href="#id-1.15.16.6">#</a></h6></div><div class="table-contents"><table class="table" summary="OLTP Data" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /><col class="col_4" /><col class="col_5" /><col class="col_6" /><col class="col_7" /><col class="col_8" /><col class="col_9" /></colgroup><thead><tr><th align="left" valign="top">Data Protection</th><th align="left" valign="top">Protocol</th><th align="left" valign="top">IO Size</th><th align="left" valign="top">Write BW (MiB/s)</th><th align="left" valign="top">Write IOPS</th><th align="left" valign="top">Write Avg Latency (ms)</th><th align="left" valign="top">Read BW (MiB/s)</th><th align="left" valign="top">Read IOPS</th><th align="left" valign="top">Read Avg Latency (ms)</th></tr></thead><tbody><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>CephFS</p></td><td align="left" valign="top"><p>8K</p></td><td align="left" valign="top"><p>6</p></td><td align="left" valign="top"><p>864.3</p></td><td align="left" valign="top"><p>17.0</p></td><td align="left" valign="top"><p>27</p></td><td align="left" valign="top"><p>3462.6</p></td><td align="left" valign="top"><p>14.1</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>CephFS</p></td><td align="left" valign="top"><p>8K</p></td><td align="left" valign="top"><p>2</p></td><td align="left" valign="top"><p>307.4</p></td><td align="left" valign="top"><p>64.3</p></td><td align="left" valign="top"><p>9</p></td><td align="left" valign="top"><p>1240.7</p></td><td align="left" valign="top"><p>35.9</p></td></tr><tr><td align="left" valign="top"><p>3xRep</p></td><td align="left" valign="top"><p>RBD</p></td><td align="left" valign="top"><p>8K</p></td><td align="left" valign="top"><p>2</p></td><td align="left" valign="top"><p>263.7</p></td><td align="left" valign="top"><p>12.6</p></td><td align="left" valign="top"><p>8</p></td><td align="left" valign="top"><p>1064.0</p></td><td align="left" valign="top"><p>11.9</p></td></tr><tr><td align="left" valign="top"><p>EC2+2</p></td><td align="left" valign="top"><p>RBD</p></td><td align="left" valign="top"><p>8K</p></td><td align="left" valign="top"><p>3</p></td><td align="left" valign="top"><p>385.1</p></td><td align="left" valign="top"><p>64.2</p></td><td align="left" valign="top"><p>12</p></td><td align="left" valign="top"><p>1553.1</p></td><td align="left" valign="top"><p>19.8</p></td></tr></tbody></table></div></div></div></div></div></div><div class="page-bottom"><div id="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span id="_share-fb" class="bottom-button">Facebook</span><span class="spacer"> • </span><span id="_share-gp" class="bottom-button">Google+</span><span class="spacer"> • </span><span id="_share-tw" class="bottom-button">Twitter</span><span class="spacer"> • </span><span id="_share-mail" class="bottom-button">E-Mail</span></span></div><div class="print"><span id="_print-button" class="bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2020 
        SUSE</p><ul><li><a href="http://www.suse.com/company/careers/" target="_top">Careers</a></li><li><a href="http://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="http://www.suse.com/company/" target="_top">About</a></li><li><a href="http://www.suse.com/ContactsOffices/contacts_offices.jsp" target="_top">Contact Us</a></li></ul></div></div></body></html>