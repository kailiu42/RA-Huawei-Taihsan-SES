<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>SUSE® Enterprise Storage 6 on Huawei Taishan Implementation Guide</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" /><meta name="generator" content="DAPS 3.0.0 (https://opensuse.github.io/daps) using SUSE XSL Stylesheets 2.0.15 (based on DocBook XSL Stylesheets 1.79.2)" /><meta name="book-title" content="SUSE® Enterprise Storage 6 on Huawei Taishan Implementation Guide" />

<script type="text/javascript">

var protocol = window.location.protocol.toLowerCase();
if ( protocol != 'file:' ) {
  var agent = navigator.userAgent.toLowerCase();
  var wanted = ( protocol == 'https:') ? 'https' : 'http';
  var file = 'fonts.css';
  document.write('<link rel="stylesheet" type="text/css" href="' + wanted + '://static.opensuse.org/fonts/'+ file +'"></link>');
}
else {
   document.write('<link rel="stylesheet" type="text/css" href="static/css/fonts-onlylocal.css"></link>');
}

</script><noscript><link rel="stylesheet" type="text/css" href="http://static.opensuse.org/fonts/fonts.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="single offline js-off"><div class="bypass-block"><a href="#_content">Jump to content</a></div><div id="_outer-wrap"><div id="_white-bg"><div id="_header"><div id="_logo"><img src="static/images/logo.png" alt="Logo" /></div><div class="crumbs inactive"><a class="single-crumb" href="#id-1" accesskey="c"><span class="single-contents-icon"></span>SUSE® Enterprise Storage 6 on Huawei Taishan Implementation Guide</a><div class="bubble-corner active-contents"></div></div><div class="clearme"></div></div></div><div id="_fixed-header-wrap" class="inactive"><div id="_fixed-header"><div class="crumbs inactive"><a class="single-crumb" href="#id-1" accesskey="c"><span class="single-contents-icon"></span>Show Contents: SUSE® Enterprise Storage 6 on Huawei Taishan Implementation Guide</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="clearme"></div></div><div class="clearme"></div></div><div class="active-contents bubble"><div class="bubble-container"><div id="_bubble-toc"><ol><li class="inactive"><a href="#_introduction"><span class="number">1 </span><span class="name">Introduction</span></a></li><li class="inactive"><a href="#_target_audience"><span class="number">2 </span><span class="name">Target Audience</span></a></li><li class="inactive"><a href="#_hardware_software"><span class="number">3 </span><span class="name">Hardware &amp; Software</span></a></li><li class="inactive"><a href="#_business_problem_and_business_value"><span class="number">4 </span><span class="name">Business Problem and Business Value</span></a></li><li class="inactive"><a href="#_architectural_overview"><span class="number">5 </span><span class="name">Architectural Overview</span></a></li><li class="inactive"><a href="#_suse_software_components"><span class="number">6 </span><span class="name">SUSE Software Components</span></a></li><li class="inactive"><a href="#_deployment"><span class="number">7 </span><span class="name">Deployment</span></a></li><li class="inactive"><a href="#_tips"><span class="number">8 </span><span class="name">Tips</span></a></li><li class="inactive"><a href="#_conclusion"><span class="number">9 </span><span class="name">Conclusion</span></a></li><li class="inactive"><a href="#_references_and_resources"><span class="number">10 </span><span class="name">References and Resources</span></a></li><li class="inactive"><a href="#appendix-policy-cfg"><span class="number">A </span><span class="name">policy.cfg example</span></a></li><li class="inactive"><a href="#appendix-drive-groups-yml"><span class="number">B </span><span class="name">drive_groups.yml example</span></a></li></ol></div><div class="clearme"></div></div></div></div><div id="_toc-bubble-wrap"></div><div id="_content" class=""><div class="documentation"><div xml:lang="en" class="book" id="id-1" lang="en"><div class="titlepage"><div><div><h1 class="title">SUSE® Enterprise Storage 6 on Huawei Taishan Implementation Guide</h1></div><div><span class="imprint-label">Author: </span><span class="firstname ">Kai</span> <span class="surname ">Liu</span></div><div class="date"><span class="imprint-label">Publication Date: </span>May 14 2020</div></div></div><div class="toc"><dl><dt><span class="chapter"><a href="#_introduction"><span class="number">1 </span><span class="name">Introduction</span></a></span></dt><dt><span class="chapter"><a href="#_target_audience"><span class="number">2 </span><span class="name">Target Audience</span></a></span></dt><dt><span class="chapter"><a href="#_hardware_software"><span class="number">3 </span><span class="name">Hardware &amp; Software</span></a></span></dt><dt><span class="chapter"><a href="#_business_problem_and_business_value"><span class="number">4 </span><span class="name">Business Problem and Business Value</span></a></span></dt><dd><dl><dt><span class="section"><a href="#_business_problem"><span class="number">4.1 </span><span class="name">Business Problem</span></a></span></dt><dt><span class="section"><a href="#_business_value"><span class="number">4.2 </span><span class="name">Business Value</span></a></span></dt><dt><span class="section"><a href="#_suse_enterprise_storage"><span class="number">4.3 </span><span class="name">SUSE Enterprise Storage</span></a></span></dt><dt><span class="section"><a href="#_huawei_taishan"><span class="number">4.4 </span><span class="name">Huawei Taishan</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#_architectural_overview"><span class="number">5 </span><span class="name">Architectural Overview</span></a></span></dt><dd><dl><dt><span class="section"><a href="#_solution_architecture"><span class="number">5.1 </span><span class="name">Solution Architecture</span></a></span></dt><dt><span class="section"><a href="#_networking_architecture"><span class="number">5.2 </span><span class="name">Networking Architecture</span></a></span></dt><dt><span class="section"><a href="#_networkip_address_scheme"><span class="number">5.3 </span><span class="name">Network/IP Address Scheme</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#_suse_software_components"><span class="number">6 </span><span class="name">SUSE Software Components</span></a></span></dt><dt><span class="chapter"><a href="#_deployment"><span class="number">7 </span><span class="name">Deployment</span></a></span></dt><dd><dl><dt><span class="section"><a href="#_network_considerations"><span class="number">7.1 </span><span class="name">Network Considerations</span></a></span></dt><dt><span class="section"><a href="#_hardware_considerations"><span class="number">7.2 </span><span class="name">Hardware Considerations</span></a></span></dt><dt><span class="section"><a href="#_operating_system_considerations"><span class="number">7.3 </span><span class="name">Operating System Considerations</span></a></span></dt><dt><span class="section"><a href="#_suse_enterprise_storage_installation_configuration"><span class="number">7.4 </span><span class="name">SUSE Enterprise Storage Installation &amp; Configuration</span></a></span></dt><dt><span class="section"><a href="#_post_deployment_quick_tests"><span class="number">7.5 </span><span class="name">Post-deployment Quick Tests</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#_tips"><span class="number">8 </span><span class="name">Tips</span></a></span></dt><dd><dl><dt><span class="section"><a href="#_use_a_different_ntp_server"><span class="number">8.1 </span><span class="name">Use a different NTP server</span></a></span></dt><dt><span class="section"><a href="#_copy_files_to_all_cluster_nodes"><span class="number">8.2 </span><span class="name">Copy files to all cluster nodes</span></a></span></dt><dt><span class="section"><a href="#_important_files"><span class="number">8.3 </span><span class="name">Important files</span></a></span></dt><dt><span class="section"><a href="#_how_to_completely_uninstall_the_cluster_for_reinstall"><span class="number">8.4 </span><span class="name">How to completely uninstall the cluster for reinstall</span></a></span></dt><dt><span class="section"><a href="#_how_to_get_salt_pillar_information"><span class="number">8.5 </span><span class="name">How to get salt pillar information</span></a></span></dt><dt><span class="section"><a href="#_ses_built_in_network_benchmark"><span class="number">8.6 </span><span class="name">SES built-in network benchmark</span></a></span></dt><dt><span class="section"><a href="#_ceph_built_in_osd_benchmark"><span class="number">8.7 </span><span class="name">Ceph built-in OSD benchmark</span></a></span></dt><dt><span class="section"><a href="#_ceph_built_in_pool_scope_benchmark"><span class="number">8.8 </span><span class="name">Ceph built-in pool scope benchmark</span></a></span></dt><dt><span class="section"><a href="#_interface_bonding"><span class="number">8.9 </span><span class="name">Interface bonding</span></a></span></dt><dt><span class="section"><a href="#_recommended_size_for_the_bluestores_wal_and_db_device"><span class="number">8.10 </span><span class="name">Recommended Size for the BlueStore’s WAL and DB Device</span></a></span></dt><dt><span class="section"><a href="#_offline_setup"><span class="number">8.11 </span><span class="name">Offline setup</span></a></span></dt><dt><span class="section"><a href="#_change_node_roles"><span class="number">8.12 </span><span class="name">Change node roles</span></a></span></dt><dt><span class="section"><a href="#_more_tips"><span class="number">8.13 </span><span class="name">More tips</span></a></span></dt></dl></dd><dt><span class="chapter"><a href="#_conclusion"><span class="number">9 </span><span class="name">Conclusion</span></a></span></dt><dt><span class="chapter"><a href="#_references_and_resources"><span class="number">10 </span><span class="name">References and Resources</span></a></span></dt><dt><span class="appendix"><a href="#appendix-policy-cfg"><span class="number">A </span><span class="name">policy.cfg example</span></a></span></dt><dt><span class="appendix"><a href="#appendix-drive-groups-yml"><span class="number">B </span><span class="name">drive_groups.yml example</span></a></span></dt></dl></div><div class="list-of-figures"><div class="toc-title">List of Figures</div><dl><dt><span class="figure"><a href="#img-ses-arch"><span class="number">5.1 </span><span class="name">SES Architecture</span></a></span></dt><dt><span class="figure"><a href="#img-ceph-network"><span class="number">5.2 </span><span class="name">Ceph Network Architecture</span></a></span></dt></dl></div><div class="list-of-tables"><div class="toc-title">List of Tables</div><dl><dt><span class="table"><a href="#id-1.6.5.3"><span class="number">5.1 </span><span class="name">Node Roles and Network Addresses</span></a></span></dt></dl></div><div class="chapter " id="_introduction"><div class="titlepage"><div><div><h1 class="title"><span class="number">1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Introduction</span> <a title="Permalink" class="permalink" href="#_introduction">#</a></h1></div></div></div><div class="line"></div><p>The objective of this guide is to present a step-by-step guide on how to implement SUSE Enterprise Storage 6 on the Huawei Taishan platform. It is suggested that the document be read in its entirety, along with the supplemental appendix information before attempting the process.</p><p>The deployment presented in this guide aligns with architectural best practices and will support the implementation of all currently supported protocols as identified in the SUSE Enterprise Storage documentation.</p><p>Upon completion of the steps in this document, a working SUSE Enterprise Storage 6 cluster will be operational as described in the <a class="link" href="https://www.suse.com/documentation/ses-5/book_storage_admin/data/book_storage_admin.html" target="_blank">SUSE Enterprise Storage Deployment and Administration Guide</a>.</p></div><div class="chapter " id="_target_audience"><div class="titlepage"><div><div><h1 class="title"><span class="number">2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Target Audience</span> <a title="Permalink" class="permalink" href="#_target_audience">#</a></h1></div></div></div><div class="line"></div><p>This reference architecture is targeted at administrators who deploy software defined storage solutions within their data centers and making the different storage services accessible to their own customer base. By following this document as well as those referenced herein, the administrator should have a full view of the SUSE Enterprise Storage architecture, deployment and administrative tasks, with a specific set of recommendations for deployment of the hardware and networking platform.</p></div><div class="chapter " id="_hardware_software"><div class="titlepage"><div><div><h1 class="title"><span class="number">3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Hardware &amp; Software</span> <a title="Permalink" class="permalink" href="#_hardware_software">#</a></h1></div></div></div><div class="line"></div><p>The recommended architecture for SUSE Enterprise Storage on Huawei Taishan leverages two models of Huawei servers. The role and functionality of each type of system within the SUSE Enterprise Storage environment will be explained in more detail in the <a class="xref" href="#_architectural_overview" title="Chapter 5. Architectural Overview">Chapter 5, <em>Architectural Overview</em></a> section.</p><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">Storage Nodes: </span><a title="Permalink" class="permalink" href="#id-1.4.3">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p><a class="link" href="https://e.huawei.com/uk/products/servers/taishan-server/taishan-5280-v2" target="_blank">Huawei 5280 v2</a></p></li></ul></div><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">Admin, monitor, and protocol gateways: </span><a title="Permalink" class="permalink" href="#id-1.4.4">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p><a class="link" href="https://e.huawei.com/uk/products/servers/taishan-server/taishan-2280-v2" target="_blank">Huawei 2280 v2</a></p></li></ul></div><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">Switches: </span><a title="Permalink" class="permalink" href="#id-1.4.5">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p>Huawei  10Gb. Higher speed model is recommended if high speed NIC is installed.</p></li></ul></div><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">Software: </span><a title="Permalink" class="permalink" href="#id-1.4.6">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p>SUSE Enterprise Storage 6</p></li><li class="listitem "><p>SUSE Linux Enterprise Server 15 SP1</p></li></ul></div></div><div class="chapter " id="_business_problem_and_business_value"><div class="titlepage"><div><div><h1 class="title"><span class="number">4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Business Problem and Business Value</span> <a title="Permalink" class="permalink" href="#_business_problem_and_business_value">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#_business_problem"><span class="number">4.1 </span><span class="name">Business Problem</span></a></span></dt><dt><span class="section"><a href="#_business_value"><span class="number">4.2 </span><span class="name">Business Value</span></a></span></dt><dt><span class="section"><a href="#_suse_enterprise_storage"><span class="number">4.3 </span><span class="name">SUSE Enterprise Storage</span></a></span></dt><dt><span class="section"><a href="#_huawei_taishan"><span class="number">4.4 </span><span class="name">Huawei Taishan</span></a></span></dt></dl></div></div><div class="sect1" id="_business_problem"><div class="titlepage"><div><div><h2 class="title" id="_business_problem"><span class="number">4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Business Problem</span> <a title="Permalink" class="permalink" href="#_business_problem">#</a></h2></div></div></div><p>Customers of all sizes face a major storage challenge: while the overall cost per Terabyte of physical storage has gone down over the years, a data growth explosion took place driven by the need to access and leverage new data sources (e.g. external sources such as social media) and the ability to "manage" new data types (e.g. unstructured or object data). These ever increasing "data lakes" need different access methods: File, Block, or Object.</p><p>Addressing these challenges with legacy storage solutions would require either a number of specialized products (usually driven by access method) with traditional protection schemes (e.g. RAID). These solutions struggle when scaling from Terabytes to Petabytes at reasonable cost and performance levels.</p></div><div class="sect1" id="_business_value"><div class="titlepage"><div><div><h2 class="title" id="_business_value"><span class="number">4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Business Value</span> <a title="Permalink" class="permalink" href="#_business_value">#</a></h2></div></div></div><p>This software defined storage solution enables transformation of the enterprise infrastructure by providing a unified platform where structured and unstructured data can co-exist and be accessed as files, blocks, or objects depending on the application requirements. The combination of open-source software (Ceph) and industry standard servers reduce cost while providing the on-ramp to unlimited scalability needed to keep up with future demands.</p></div><div class="sect1" id="_suse_enterprise_storage"><div class="titlepage"><div><div><h2 class="title" id="_suse_enterprise_storage"><span class="number">4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SUSE Enterprise Storage</span> <a title="Permalink" class="permalink" href="#_suse_enterprise_storage">#</a></h2></div></div></div><p>SUSE Enterprise Storage delivers a highly scalable, resilient, self-healing storage system designed for large scale environments ranging from hundreds of Terabytes to Petabytes. This software defined storage product can reduce IT costs by leveraging industry standard servers to present unified storage servicing block, file, and object protocols. Having storage that can meet the current needs and requirements of the data center while supporting topologies and protocols demanded by new web-scale applications, enables administrators to support the ever-increasing storage requirements of the enterprise with ease.</p></div><div class="sect1" id="_huawei_taishan"><div class="titlepage"><div><div><h2 class="title" id="_huawei_taishan"><span class="number">4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Huawei Taishan</span> <a title="Permalink" class="permalink" href="#_huawei_taishan">#</a></h2></div></div></div><p>Huawei Taishan servers provide a cost effective and scalable platform for the deployment of SUSE Enterprise Storage. These platforms unlocks the full potential of the Kunpeng CPU, raising the bar of SPECint benchmark by 25%, with up to 128 cores, 32 DDR4 DIMM slots, PCIe 4.0 support, and 100 GE LOM.</p><p>Featuring models tailored for computing, storage, or balanced needs, Taishan is perfect for demanding workloads such as big data analytics, database acceleration, high-performance computing, and cloud services. Taishan servers empower data centers with the ultimate efficiency.</p></div></div><div class="chapter " id="_architectural_overview"><div class="titlepage"><div><div><h1 class="title"><span class="number">5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Architectural Overview</span> <a title="Permalink" class="permalink" href="#_architectural_overview">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#_solution_architecture"><span class="number">5.1 </span><span class="name">Solution Architecture</span></a></span></dt><dt><span class="section"><a href="#_networking_architecture"><span class="number">5.2 </span><span class="name">Networking Architecture</span></a></span></dt><dt><span class="section"><a href="#_networkip_address_scheme"><span class="number">5.3 </span><span class="name">Network/IP Address Scheme</span></a></span></dt></dl></div></div><p>This architecture overview section complements the <a class="link" href="https://www.suse.com/media/white-paper/suse_enterprise_storage_technical_overview_wp.pdf" target="_blank">SUSE Enterprise Storage Technical Overview</a> document available online which presents the concepts behind software defined storage and Ceph as well as a quick start guide (non-platform specific).</p><div class="sect1" id="_solution_architecture"><div class="titlepage"><div><div><h2 class="title" id="_solution_architecture"><span class="number">5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Solution Architecture</span> <a title="Permalink" class="permalink" href="#_solution_architecture">#</a></h2></div></div></div><p>SUSE Enterprise Storage provides unified block, file, and object access based on Ceph. Ceph is a distributed storage solution designed for scalability, reliability and performance. A critical component of Ceph is the RADOS object storage. RADOS enables a number of storage nodes to function together to store and retrieve data from the cluster using object storage techniques. The result is a storage solution that is abstracted from the hardware.</p><p>Ceph supports both native and traditional client access. The native clients are aware of the storage topology and communicate directly with the storage daemons over the public network, resulting in horizontally scaling performance. Non-native protocols, such as iSCSI, S3, and NFS require the use of gateways. While these gateways may be thought of as a limiting factor, the iSCSI and S3 gateways can scale horizontally using load balancing techniques.</p><div class="figure" id="img-ses-arch"><div class="figure-contents"><div class="mediaobject" align="center"><a xmlns="" href="images/SES-Reference-Architecture.png"><img src="images/SES-Reference-Architecture.png" width="" alt="SES Architecture" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 5.1: </span><span class="name">SES Architecture </span><a title="Permalink" class="permalink" href="#img-ses-arch">#</a></h6></div></div><p>In addition to the required network infrastructure, the minimum SUSE Enterprise Storage cluster comprises of a minimum of one administration server (physical or virtual), four object storage device nodes (OSDs), and three monitor nodes (MONs).</p><p>Please refer to the <a class="link" href="https://documentation.suse.com/en-us/ses/6/single-html/ses-deployment/#storage-bp-hwreq" target="_blank">SES 6 Deployment Guide</a> for more details on the hardware requirement.</p><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">Specific to this implementation: </span><a title="Permalink" class="permalink" href="#id-1.6.3.7">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p>One system is deployed as the administrative server. It is the Salt Master and hosts the SUSE Enterprise Storage Administration Interface, dashboard, which is the central management system which supports the cluster.</p></li><li class="listitem "><p>Three systems are deployed as monitor (MONs) nodes. Monitor nodes maintain information about the cluster health state, a map of the other monitor nodes and a CRUSH map. They also keep history of changes performed to the cluster.</p></li><li class="listitem "><p>It is strongly recommended to deploy monitors and other services on dedicated nodes. However it is also possible to deploy the monitors on the OSD nodes if there are enough hardware resources. This is the case in this specific reference setup.</p></li><li class="listitem "><p>The RADOS gateway provides S3 and Swift based access methods to the cluster. These nodes are generally situated behind a load balancer infrastructure to provide redundancy and scalability. It is important to note that the load generated by the RADOS gateway can consume a significant amount of compute and memory resources making the minimum recommended configuration contain 6-8 CPU cores and 32GB of RAM.</p></li><li class="listitem "><p>SUSE Enterprise Storage requires a minimum of four systems as storage nodes. The storage nodes contain individual storage devices that are each assigned an Object Storage Daemon (OSD). The OSD daemon assigned to the device stores data and manages the data replication and rebalancing processes. OSD daemons also communicate with the monitor (MON) nodes and provide them with the state of the other OSD daemons.</p></li></ul></div></div><div class="sect1" id="_networking_architecture"><div class="titlepage"><div><div><h2 class="title" id="_networking_architecture"><span class="number">5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Networking Architecture</span> <a title="Permalink" class="permalink" href="#_networking_architecture">#</a></h2></div></div></div><p>A software-defined solution is only as reliable as its slowest and least redundant component. This makes it important to design and implement a robust, high performance storage network infrastructure. From a network perspective for Ceph, this translates into:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Separation of cluster internal and client-facing public network traffic. This isolates Ceph OSD daemon replication activities from Ceph clients. This may be achieved through separate physical networks or through use of VLANs.</p></li><li class="listitem "><p>Redundancy and capacity in the form of bonded network interfaces connected to switches.</p></li></ul></div><p><a class="xref" href="#img-ceph-network" title="Ceph Network Architecture">Figure 5.2, “Ceph Network Architecture”</a> shows the logical layout of the Ceph cluster implementation.</p><div class="figure" id="img-ceph-network"><div class="figure-contents"><div class="mediaobject" align="center"><a xmlns="" href="images/Ceph-Network.png"><img src="images/Ceph-Network.png" width="" alt="Ceph Network Architecture" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 5.2: </span><span class="name">Ceph Network Architecture </span><a title="Permalink" class="permalink" href="#img-ceph-network">#</a></h6></div></div></div><div class="sect1" id="_networkip_address_scheme"><div class="titlepage"><div><div><h2 class="title" id="_networkip_address_scheme"><span class="number">5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network/IP Address Scheme</span> <a title="Permalink" class="permalink" href="#_networkip_address_scheme">#</a></h2></div></div></div><p>Specific to this implementation, the following naming and addressing scheme were utilized.</p><div class="table" id="id-1.6.5.3"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 5.1: </span><span class="name">Node Roles and Network Addresses </span><a title="Permalink" class="permalink" href="#id-1.6.5.3">#</a></h6></div><div class="table-contents"><table class="table" summary="Node Roles and Network Addresses" border="1"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /><col class="col_4" /></colgroup><thead><tr><th align="left" valign="top">Role</th><th align="left" valign="top">Hostname</th><th align="left" valign="top">Public Network</th><th align="left" valign="top">Cluster Network</th></tr></thead><tbody><tr><td align="left" valign="top"><p>Admin</p></td><td align="left" valign="top"><p>admin.example.com</p></td><td align="left" valign="top"><p>10.1.1.3</p></td><td align="left" valign="top"><p>N/A</p></td></tr><tr><td align="left" valign="top"><p>Monitor</p></td><td align="left" valign="top"><p>ceph1.example.com</p></td><td align="left" valign="top"><p>10.1.1.4</p></td><td align="left" valign="top"><p>N/A</p></td></tr><tr><td align="left" valign="top"><p>Monitor</p></td><td align="left" valign="top"><p>ceph2.example.com</p></td><td align="left" valign="top"><p>10.1.1.5</p></td><td align="left" valign="top"><p>N/A</p></td></tr><tr><td align="left" valign="top"><p>Monitor</p></td><td align="left" valign="top"><p>ceph3.example.com</p></td><td align="left" valign="top"><p>10.1.1.6</p></td><td align="left" valign="top"><p>N/A</p></td></tr><tr><td align="left" valign="top"><p>OSD Node</p></td><td align="left" valign="top"><p>ceph1.example.com</p></td><td align="left" valign="top"><p>10.1.1.4</p></td><td align="left" valign="top"><p>10.2.1.4</p></td></tr><tr><td align="left" valign="top"><p>OSD Node</p></td><td align="left" valign="top"><p>ceph2.example.com</p></td><td align="left" valign="top"><p>10.1.1.5</p></td><td align="left" valign="top"><p>10.2.1.5</p></td></tr><tr><td align="left" valign="top"><p>OSD Node</p></td><td align="left" valign="top"><p>ceph3.example.com</p></td><td align="left" valign="top"><p>10.1.1.6</p></td><td align="left" valign="top"><p>10.2.1.6</p></td></tr><tr><td align="left" valign="top"><p>OSD Node</p></td><td align="left" valign="top"><p>ceph4.example.com</p></td><td align="left" valign="top"><p>10.1.1.7</p></td><td align="left" valign="top"><p>10.2.1.7</p></td></tr></tbody></table></div></div></div></div><div class="chapter " id="_suse_software_components"><div class="titlepage"><div><div><h1 class="title"><span class="number">6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SUSE Software Components</span> <a title="Permalink" class="permalink" href="#_suse_software_components">#</a></h1></div></div></div><div class="line"></div><p>In this section, the focus is on the SUSE components: SUSE Linux Enterprise Server (SLES), SUSE Enterprise Storage (SES), and the Repository Mirroring Tool (RMT).</p><div class="variablelist "><dl class="variablelist"><dt id="id-1.7.3.1"><span class="term ">SUSE Linux Enterprise Server</span></dt><dd><p>A world class secure, open source server operating system, equally adept at powering physical, virtual, or cloud-based mission-critical workloads. SUSE Linux Enterprise Server 15 SP1 further raises the bar in helping organizations to accelerate innovation, enhance system reliability, meet tough security requirements and adapt to new technologies.</p></dd><dt id="id-1.7.3.2"><span class="term ">Repository Mirroring Tool</span></dt><dd><p>Allows enterprise customers to optimize the management of SUSE Linux Enterprise (and other products such as SUSE Enterprise Storage) software updates and subscription entitlements. It establishes a proxy system for SUSE Customer Center with repository and registration targets.</p></dd><dt id="id-1.7.3.3"><span class="term ">SUSE Enterprise Storage</span></dt><dd><p>Provided as an product on top of SUSE Linux Enterprise Server, this intelligent software-defined storage solution, powered by Ceph technology with enterprise engineering and support from SUSE enables customers to transform enterprise infrastructure to reduce costs while providing unlimited scalability.</p></dd></dl></div></div><div class="chapter " id="_deployment"><div class="titlepage"><div><div><h1 class="title"><span class="number">7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deployment</span> <a title="Permalink" class="permalink" href="#_deployment">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#_network_considerations"><span class="number">7.1 </span><span class="name">Network Considerations</span></a></span></dt><dt><span class="section"><a href="#_hardware_considerations"><span class="number">7.2 </span><span class="name">Hardware Considerations</span></a></span></dt><dt><span class="section"><a href="#_operating_system_considerations"><span class="number">7.3 </span><span class="name">Operating System Considerations</span></a></span></dt><dt><span class="section"><a href="#_suse_enterprise_storage_installation_configuration"><span class="number">7.4 </span><span class="name">SUSE Enterprise Storage Installation &amp; Configuration</span></a></span></dt><dt><span class="section"><a href="#_post_deployment_quick_tests"><span class="number">7.5 </span><span class="name">Post-deployment Quick Tests</span></a></span></dt></dl></div></div><p>This deployment section should be seen as a supplement to the <a class="link" href="https://documentation.suse.com/" target="_blank">SUSE official documentations</a>. Please refer to the <a class="xref" href="#_references_and_resources" title="Chapter 10. References and Resources">Chapter 10, <em>References and Resources</em></a> for the list of related SUSE documents.</p><p>It is assumed that a Subscription Management Tool server or a Repository Mirroring Server exists within the environment. If not, please follow the <a class="link" href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-rmt/#book-rmt" target="_blank">RMT Guide</a> and <a class="xref" href="#_offline_setup" title="8.11. Offline setup">Section 8.11, “Offline setup”</a> to make one available.</p><p>In this document we use <code class="literal">example.com</code> as the domain name for the nodes, replace it with your real domain name in your own installation.</p><div class="sect1" id="_network_considerations"><div class="titlepage"><div><div><h2 class="title" id="_network_considerations"><span class="number">7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Network Considerations</span> <a title="Permalink" class="permalink" href="#_network_considerations">#</a></h2></div></div></div><p>The following considerations for the network configuration should be attended to:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Ensure that all network switches are updated with consistent firmware versions.</p></li><li class="listitem "><p>Depending on the network interface bonding mode used on the servers, corresponding switch port configuration may be required. Please consult your network administrator for that topic as it’s out of the scope of this document.</p></li><li class="listitem "><p>Network IP addressing and IP ranges need proper planning. In optimal environments, a dedicated storage subnet should be used for all SUSE Enterprise Storage nodes on the primary network, with a separate, dedicated subnet for the cluster network. Depending on the size of the installation, ranges larger than /24 may be required. When planning the network, current size as well as future growth should be taken into consideration.</p></li></ul></div></div><div class="sect1" id="_hardware_considerations"><div class="titlepage"><div><div><h2 class="title" id="_hardware_considerations"><span class="number">7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Hardware Considerations</span> <a title="Permalink" class="permalink" href="#_hardware_considerations">#</a></h2></div></div></div><p>The following considerations for the hardware platforms should be attended to:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Configure the system to run in performance mode if you prefer performance over power efficiency. To change that, reboot the system, press <span class="keycap">Del</span> when prompted during system initializing to boot into the BIOS setup menu. Then select <span class="guimenu ">Advanced</span> › <span class="guimenu ">Performance Config</span> › <span class="guimenu ">Power Policy</span>, select <span class="guimenu ">Performance</span> for best performance or <span class="guimenu ">Efficiency</span> for power efficiency.</p></li><li class="listitem "><p>In case you don’t see the SUSE Installer screen after boot up from the SUSE installation medium, check the BIOS option <span class="guimenu ">Advanced</span> › <span class="guimenu ">MISC Config</span> › <span class="guimenu ">Support SPCR</span> and set it to <span class="guimenu ">Disabled</span>.</p></li><li class="listitem "><p>A RAID-1 volume consists of two 600GB SAS hard drive is enough for the OS disk.</p></li><li class="listitem "><p>If hard drives are connected to hardware RAID controller(s) with hardware write cache, configure each of them as individual RAID-0 volume and make sure hardware caching is enabled.</p></li><li class="listitem "><p>Try to balance the drives across controllers, ports, and enclosures. Avoid making one part of the I/O subsystem busy while leaving other parts idle.</p></li><li class="listitem "><p>If SAS/SATA SSDs are installed, make sure to attach then to a dedicated HBA or RAID controller rather than to the controller that already has many HDDs attached.</p></li></ul></div></div><div class="sect1" id="_operating_system_considerations"><div class="titlepage"><div><div><h2 class="title" id="_operating_system_considerations"><span class="number">7.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Operating System Considerations</span> <a title="Permalink" class="permalink" href="#_operating_system_considerations">#</a></h2></div></div></div><p>The following considerations for the Operating System should be attended to:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>The underlying OS for SES 6 is SUSE Linux Enterprise Server 15 SP1. Other OS versions are not supported. During installation, make sure below addon modules are selected.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Base system Module</p></li><li class="listitem "><p>Server Applications Module</p></li><li class="listitem "><p>SUSE Enterprise Storage 6</p><div id="id-1.8.7.3.1.2.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>SUSE Enterprise Storage is a paid product in it’s own. You need to purchase the subscription before you can install it as an add-on on top of SUSE Linux Enterprise Server.</p></div></li></ul></div></li><li class="listitem "><p>During installation, don’t select any GUI components such as X-Window system, GNOME or KDE, as they are not needed to run the storage service.</p></li><li class="listitem "><p>It is highly recommended to register the systems to an update server to install the latest updates available, helping to ensure the best experience possible. The systems could be registered directly to SUSE Customer Center if it is a small cluster, or could be registered to a local SMT or RMT server when the cluster is large. Installing updates from a local SMT/RMT server will dramatically reduce the time required for updates to be downloaded to all nodes.</p><div id="id-1.8.7.3.3.2" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>Refer to <a class="link" href="https://documentation.suse.com/en-us/sles/15-SP1/single-html/SLES-rmt/" target="_blank">Repository Mirroring Tool Guide</a> for how to setup a RMT server.</p></div></li><li class="listitem "><p>Ensure that the operating system is installed on the correct device. Especially on OSD nodes, the installer may not choose the right one from many available drives.</p></li><li class="listitem "><p>Hostnames of all nodes should be properly configured. Full hostname (i.e. with domain name) should always be assigned for each node or else the deployment may fail. Make sure <code class="literal">hostname -s</code>, <code class="literal">hostname -f</code> and <code class="literal">hostname -i</code> commands return proper results for short hostname (without dots), full hostname and IP addresses. Each node must also be able to resolve hostname of all nodes, including its own name.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>For a rather small cluster, hosts files can be used for name resolution. Also see <a class="xref" href="#_copy_files_to_all_cluster_nodes" title="8.2. Copy files to all cluster nodes">Section 8.2, “Copy files to all cluster nodes”</a> for how to conveniently keep the hosts on all nodes in sync.</p></li><li class="listitem "><p>Having a DNS server is recommended for a larger cluster. See the <a class="link" href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-admin/#cha-dns" target="_blank">SUSE Linux Enterprise Server Administration Guide</a> for how to setup a DNS server.</p></li></ul></div></li><li class="listitem "><p>Do ensure that NTP is configured to point to a valid, physical NTP server. This is critical for SUSE Enterprise Storage to function properly, and failure to do so can result in an unhealthy or non-functional cluster. And keep in mind that the NTP service is not designed to be run on an virtualized environment, so make sure the NTP server been used is an physical machine or it may cause strange clock drifting problem.</p></li></ul></div></div><div class="sect1" id="_suse_enterprise_storage_installation_configuration"><div class="titlepage"><div><div><h2 class="title" id="_suse_enterprise_storage_installation_configuration"><span class="number">7.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SUSE Enterprise Storage Installation &amp; Configuration</span> <a title="Permalink" class="permalink" href="#_suse_enterprise_storage_installation_configuration">#</a></h2></div></div></div><div class="sect2" id="_software_deployment_configuration_deepsea_and_salt"><div class="titlepage"><div><div><h3 class="title" id="_software_deployment_configuration_deepsea_and_salt"><span class="number">7.4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Software Deployment Configuration (Deepsea and Salt)</span> <a title="Permalink" class="permalink" href="#_software_deployment_configuration_deepsea_and_salt">#</a></h3></div></div></div><p>Salt, along with DeepSea, is a stack of components that help deploy and manage server infrastructure. It is very scalable, fast, and relatively easy to get running.</p><p>There are three key Salt imperatives that need to be followed:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>The Salt Master is the host that controls the entire cluster deployment. Ceph itself should NOT be running on the master as all resources should be dedicated to Salt master services. In our scenario, we used the Admin host as the Salt master.</p></li><li class="listitem "><p>Salt minions are nodes controlled by Salt master. OSD, monitor, and gateway nodes are all Salt minions in this installation.</p></li><li class="listitem "><p>Salt minions need to correctly resolve the Salt master’s host name.</p></li></ul></div><p>Deepsea consists of series of Salt files to automate the deployment and management of a Ceph cluster. It consolidates the administrator’s decision making in a single location around cluster layout, node role assignment and drive assignment. Deepsea collects each set of tasks into a goal or stage.</p><p>The following steps, performed in order, were used for this reference implementation. All commands were run by root user.</p></div><div class="sect2" id="_prepare_all_nodes"><div class="titlepage"><div><div><h3 class="title" id="_prepare_all_nodes"><span class="number">7.4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prepare All Nodes</span> <a title="Permalink" class="permalink" href="#_prepare_all_nodes">#</a></h3></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Install salt master on the Admin node:</p><div class="verbatim-wrap"><pre class="screen">zypper in salt-master</pre></div></li><li class="listitem "><p>Start the salt-master service and enable start on boot:</p><div class="verbatim-wrap"><pre class="screen">systemctl enable --now salt-master.service</pre></div></li><li class="listitem "><p>Install the salt-minion on all cluster nodes (including the Admin):</p><div class="verbatim-wrap"><pre class="screen">zypper in salt-minion</pre></div></li><li class="listitem "><p>Configure all minions to connect to the Salt master:</p><p>Create a new file <code class="literal">/etc/salt/minion.d/master.conf</code> with the following content:</p><div class="verbatim-wrap"><pre class="screen">master: admin.example.com</pre></div></li><li class="listitem "><p>Restart the salt-minion service and enable it:</p><div class="verbatim-wrap"><pre class="screen">systemctl restart salt-minion.service
systemctl enable salt-minion.service</pre></div></li><li class="listitem "><p>List Salt fingerprints on all the minions:</p><div class="verbatim-wrap"><pre class="screen">salt-call --local key.finger</pre></div></li><li class="listitem "><p>List all incoming minion fingerprints on the Salt master, verify them against the fingerprints on each minions to make sure they all match. If they do, accept all Salt keys on the Salt master:</p><div class="verbatim-wrap"><pre class="screen">salt-key -F
salt-key --accept-all
salt-key --list-all</pre></div></li><li class="listitem "><p>Verify if Salt works properly by "ping" each minions from the Salt master. They should all return True on success:</p><div class="verbatim-wrap"><pre class="screen">salt '*' test.ping</pre></div></li><li class="listitem "><p>Now check and make sure the time on all nodes are the same. In later stage Deepsea will setup all nodes to synchronize time from the admin node, but before that is done, strange errors may occur if time on each node are largely out of sync. So it’s better to set all nodes to the same time manually first. For example, run below command on your admin node:</p><div class="verbatim-wrap"><pre class="screen">salt '*' cmd.run 'date -s "2020-03-19 17:30:00"'</pre></div><p>Use your actual time in the same format when running the command. It doesn’t have to be super accurate, as later all nodes will be synchronized by the chrony time service.</p></li></ol></div></div><div class="sect2" id="_prepare_osd_disks_on_osd_nodes"><div class="titlepage"><div><div><h3 class="title" id="_prepare_osd_disks_on_osd_nodes"><span class="number">7.4.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Prepare OSD Disks on OSD Nodes</span> <a title="Permalink" class="permalink" href="#_prepare_osd_disks_on_osd_nodes">#</a></h3></div></div></div><p>If the OSD nodes were used in a prior installation, or the disks are used by other applications before, zap ALL the OSD disks first.</p><div id="id-1.8.8.4.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>This must be done on all the OSD disks that were used before, or else the deployment may fail when activating OSDs.</p></div><div id="id-1.8.8.4.4" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.png" /><h6>Warning</h6><p>Below commands should not be copied and executed on your installation blindly. The device names used below are just examples, you need to change them to match only the OSD disks in your own installation. Failed to use the correct device name may erase your OS disk or other disks that may hold valuable data.</p></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Wipe the beginning of each partition:</p><div class="verbatim-wrap"><pre class="screen">for partition in /dev/sdX[0-9]*
do
  dd if=/dev/zero of=$partition bs=4096 count=1 oflag=direct
done</pre></div></li><li class="listitem "><p>Wipe the beginning of the drive:</p><div class="verbatim-wrap"><pre class="screen">dd if=/dev/zero of=/dev/sdX bs=512 count=34 oflag=direct</pre></div></li><li class="listitem "><p>Wipe the end of the drive:</p><div class="verbatim-wrap"><pre class="screen">dd if=/dev/zero of=/dev/sdX bs=512 count=33 \
  seek=$((`blockdev --getsz /dev/sdX` - 33)) oflag=direct</pre></div></li></ol></div></div><div class="sect2" id="_install_and_configure_deepsea_on_admin_node"><div class="titlepage"><div><div><h3 class="title" id="_install_and_configure_deepsea_on_admin_node"><span class="number">7.4.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Install and Configure Deepsea on Admin Node</span> <a title="Permalink" class="permalink" href="#_install_and_configure_deepsea_on_admin_node">#</a></h3></div></div></div><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Install deepsea package on Admin node:</p><div class="verbatim-wrap"><pre class="screen"># zypper in deepsea</pre></div></li><li class="listitem "><p>Check <span class="emphasis"><em>/srv/pillar/ceph/master_minion.sls</em></span> for correctness.</p></li><li class="listitem "><p>Check <span class="emphasis"><em>/srv/pillar/ceph/deepsea_minions.sls</em></span> file, make sure the deepsea_minions option targets the correct nodes. In the usual case, it can simply be put like below to match all Salt minions in the cluster:</p><div class="verbatim-wrap"><pre class="screen">deepsea_minions: '*'</pre></div></li><li class="listitem "><p>Create <span class="emphasis"><em>/srv/pillar/ceph/stack/ceph/cluster.yml</em></span> <span id="create-cluster-yml"></span> with below options:</p><div class="verbatim-wrap"><pre class="screen">cluster_network: &lt;net/mask of the cluster network&gt;
public_network: &lt;net/mask of the public network&gt;
time_server: &lt;Address of the NTP server, if this line is omitted admin node will be used&gt;</pre></div></li></ol></div></div><div class="sect2" id="_deploy_using_deepsea"><div class="titlepage"><div><div><h3 class="title" id="_deploy_using_deepsea"><span class="number">7.4.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Deploy Using Deepsea</span> <a title="Permalink" class="permalink" href="#_deploy_using_deepsea">#</a></h3></div></div></div><p>At this point Deepsea commands can be run on the admin node to deploy the cluster.</p><div id="id-1.8.8.6.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>Each command can be run either as:</p><div class="verbatim-wrap"><pre class="screen">salt-run state.orch ceph.stage.&lt;stage name&gt;</pre></div><p>Or:</p><div class="verbatim-wrap"><pre class="screen">deepsea stage run ceph.stage.&lt;stage name&gt;</pre></div><p>The latter form is preferred as it outputs real time progress.</p></div><div class="sect3" id="_stage_0_prepare"><div class="titlepage"><div><div><h4 class="title" id="_stage_0_prepare"><span class="number">7.4.5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Stage 0: Prepare</span> <a title="Permalink" class="permalink" href="#_stage_0_prepare">#</a></h4></div></div></div><p>During this stage, all required updates are applied and your nodes may be rebooted.</p><div class="verbatim-wrap"><pre class="screen">deepsea stage run ceph.stage.0</pre></div><div id="id-1.8.8.6.4.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>If the Salt master reboots during Stage 0, you need to run Stage 0 again after it boots up.</p></div><p>Optionally, create the <span class="emphasis"><em>/var/lib/ceph</em></span> btrfs subvolume:</p><div class="verbatim-wrap"><pre class="screen">salt-run state.orch ceph.migrate.subvolume</pre></div></div><div class="sect3" id="_stage_1_discovery"><div class="titlepage"><div><div><h4 class="title" id="_stage_1_discovery"><span class="number">7.4.5.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Stage 1: Discovery</span> <a title="Permalink" class="permalink" href="#_stage_1_discovery">#</a></h4></div></div></div><p>During this stage, all hardware in your cluster is detected and necessary information are collected for the Ceph configuration.</p><div class="verbatim-wrap"><pre class="screen">deepsea stage run ceph.stage.1</pre></div><div id="id-1.8.8.6.5.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Configure cluster and public network in <span class="emphasis"><em>/srv/pillar/ceph/stack/ceph/cluster.yml</em></span> if not yet done as described in <a class="xref" href="#create-cluster-yml">Create cluster.yml</a>.</p></div><p>Now a <span class="emphasis"><em>/srv/pillar/ceph/proposals/policy.cfg</em></span> file needs to be created to instruct Deepsea on the location and configuration files to use for the different components that make up the Ceph cluster (Salt master, admin, monitor, OSD and other roles).</p><p>To do so, copy the example file to the right location then edit it to match your installation:</p><div class="verbatim-wrap"><pre class="screen">cp /usr/share/doc/packages/deepsea/examples/policy.cfg-rolebased /srv/pillar/ceph/proposals/policy.cfg</pre></div><div id="id-1.8.8.6.5.8" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>See <a class="xref" href="#appendix-policy-cfg" title="Appendix A. policy.cfg example">Appendix A, <em>policy.cfg example</em></a> for the one used when installing the cluster described in this document.</p></div></div><div class="sect3" id="_stage_2_configure"><div class="titlepage"><div><div><h4 class="title" id="_stage_2_configure"><span class="number">7.4.5.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Stage 2: Configure</span> <a title="Permalink" class="permalink" href="#_stage_2_configure">#</a></h4></div></div></div><p>During this stage necessary configuration data are prepared in particular format.</p><div class="verbatim-wrap"><pre class="screen">deepsea stage run ceph.stage.2</pre></div><div id="id-1.8.8.6.6.4" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>Use below command to check the attributes of each node:</p><div class="verbatim-wrap"><pre class="screen">salt '*' pillar.items</pre></div><p>Ensure the public and cluster network attributes are the same as configured.</p></div></div><div class="sect3" id="_define_drive_groups"><div class="titlepage"><div><div><h4 class="title" id="_define_drive_groups"><span class="number">7.4.5.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Define drive groups</span> <a title="Permalink" class="permalink" href="#_define_drive_groups">#</a></h4></div></div></div><p>DriveGroups information are defined in the file <span class="emphasis"><em>/srv/salt/ceph/configuration/files/drive_groups.yml</em></span>. It specifies what drives should be used for data device, DB device, or WAL device, and other parameters for setting up the OSDs.</p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>First take a look of all the disks on all OSD nodes:</p><div class="verbatim-wrap"><pre class="screen">salt-run disks.details</pre></div><p>It lists the vendor, model, size and type of the disks. Those information can be used to match a group of drives and assign them to different uses.</p></li><li class="listitem "><p>Now define drive groups in the drive_groups.yml file.</p><div id="id-1.8.8.6.7.3.2.2" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.png" /><h6>Tip</h6><p>See <a class="xref" href="#appendix-drive-groups-yml" title="Appendix B. drive_groups.yml example">Appendix B, <em>drive_groups.yml example</em></a> for the drive group definition used in this example cluster.
For complete information refers to the <a class="link" href="https://documentation.suse.com/en-us/ses/6/single-html/ses-deployment/#ds-drive-groups" target="_blank">Deployment Guide</a></p></div></li><li class="listitem "><p>After finished editing <span class="emphasis"><em>drive_groups.yml</em></span>, run below commands to see the result definition. Exam it carefully and make sure it meets your expectation before moving on to next step.</p><div class="verbatim-wrap"><pre class="screen">salt-run disks.list
salt-run disks.report</pre></div></li></ol></div></div><div class="sect3" id="_stage_3_deploy"><div class="titlepage"><div><div><h4 class="title" id="_stage_3_deploy"><span class="number">7.4.5.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Stage 3: Deploy</span> <a title="Permalink" class="permalink" href="#_stage_3_deploy">#</a></h4></div></div></div><p>A basic Ceph cluster with mandatory Ceph services is created in this stage.</p><div class="verbatim-wrap"><pre class="screen">deepsea stage run ceph.stage.3</pre></div><div id="id-1.8.8.6.8.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>It may take quite some time for above command to finish if your cluster is large, your have a lot of disks, or your Internet bandwidth is limited while you didn’t register the nodes to local SMT server.</p></div><p>After the above command is finished successfully, check whether the cluster is up by running:</p><div class="verbatim-wrap"><pre class="screen">ceph -s</pre></div></div><div class="sect3" id="_stage_4_services"><div class="titlepage"><div><div><h4 class="title" id="_stage_4_services"><span class="number">7.4.5.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Stage 4: Services</span> <a title="Permalink" class="permalink" href="#_stage_4_services">#</a></h4></div></div></div><p>Additional features of Ceph like iSCSI, Object Gateway and CephFS can be installed in this stage. Each is optional and up to your situation.</p><div class="verbatim-wrap"><pre class="screen">deepsea stage run ceph.stage.4</pre></div><p>After the above command finishes successfully, the SUSE Enterprise Storage cluster is considered fully deployed.</p></div></div></div><div class="sect1" id="_post_deployment_quick_tests"><div class="titlepage"><div><div><h2 class="title" id="_post_deployment_quick_tests"><span class="number">7.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Post-deployment Quick Tests</span> <a title="Permalink" class="permalink" href="#_post_deployment_quick_tests">#</a></h2></div></div></div><p>The steps below can be used to validate the overall cluster health, by creating a <code class="literal">test</code> storage pool and run some write and read test on it.</p><div class="verbatim-wrap"><pre class="screen">ceph status
ceph osd pool create test 1024
rados -p test bench 300 write --no-cleanup
rados -p test bench 300 seq</pre></div><p>Once the tests are complete, you can remove the test pool via:</p><div class="verbatim-wrap"><pre class="screen">ceph tell mon.* injectargs --mon-allow-pool-delete=true
ceph osd pool delete test test --yes-i-really-really-mean-it
ceph tell mon.* injectargs --mon-allow-pool-delete=false</pre></div></div></div><div class="chapter " id="_tips"><div class="titlepage"><div><div><h1 class="title"><span class="number">8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Tips</span> <a title="Permalink" class="permalink" href="#_tips">#</a></h1></div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#_use_a_different_ntp_server"><span class="number">8.1 </span><span class="name">Use a different NTP server</span></a></span></dt><dt><span class="section"><a href="#_copy_files_to_all_cluster_nodes"><span class="number">8.2 </span><span class="name">Copy files to all cluster nodes</span></a></span></dt><dt><span class="section"><a href="#_important_files"><span class="number">8.3 </span><span class="name">Important files</span></a></span></dt><dt><span class="section"><a href="#_how_to_completely_uninstall_the_cluster_for_reinstall"><span class="number">8.4 </span><span class="name">How to completely uninstall the cluster for reinstall</span></a></span></dt><dt><span class="section"><a href="#_how_to_get_salt_pillar_information"><span class="number">8.5 </span><span class="name">How to get salt pillar information</span></a></span></dt><dt><span class="section"><a href="#_ses_built_in_network_benchmark"><span class="number">8.6 </span><span class="name">SES built-in network benchmark</span></a></span></dt><dt><span class="section"><a href="#_ceph_built_in_osd_benchmark"><span class="number">8.7 </span><span class="name">Ceph built-in OSD benchmark</span></a></span></dt><dt><span class="section"><a href="#_ceph_built_in_pool_scope_benchmark"><span class="number">8.8 </span><span class="name">Ceph built-in pool scope benchmark</span></a></span></dt><dt><span class="section"><a href="#_interface_bonding"><span class="number">8.9 </span><span class="name">Interface bonding</span></a></span></dt><dt><span class="section"><a href="#_recommended_size_for_the_bluestores_wal_and_db_device"><span class="number">8.10 </span><span class="name">Recommended Size for the BlueStore’s WAL and DB Device</span></a></span></dt><dt><span class="section"><a href="#_offline_setup"><span class="number">8.11 </span><span class="name">Offline setup</span></a></span></dt><dt><span class="section"><a href="#_change_node_roles"><span class="number">8.12 </span><span class="name">Change node roles</span></a></span></dt><dt><span class="section"><a href="#_more_tips"><span class="number">8.13 </span><span class="name">More tips</span></a></span></dt></dl></div></div><div class="sect1" id="_use_a_different_ntp_server"><div class="titlepage"><div><div><h2 class="title" id="_use_a_different_ntp_server"><span class="number">8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Use a different NTP server</span> <a title="Permalink" class="permalink" href="#_use_a_different_ntp_server">#</a></h2></div></div></div><p>The default time server is the admin node. To change it, add</p><div class="verbatim-wrap"><pre class="screen">time_server: &lt;server address&gt;</pre></div><p>in <span class="emphasis"><em>/srv/pillar/ceph/stack/ceph/cluster.yml</em></span></p></div><div class="sect1" id="_copy_files_to_all_cluster_nodes"><div class="titlepage"><div><div><h2 class="title" id="_copy_files_to_all_cluster_nodes"><span class="number">8.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Copy files to all cluster nodes</span> <a title="Permalink" class="permalink" href="#_copy_files_to_all_cluster_nodes">#</a></h2></div></div></div><p><code class="literal">salt-cp</code> command can be used to copy files from the salt master node to minion nodes. This can be very convenient, for example, to keep /etc/hosts file in sync on all nodes.</p><div class="verbatim-wrap"><pre class="screen">salt-cp '*' /etc/hosts /etc/hosts</pre></div></div><div class="sect1" id="_important_files"><div class="titlepage"><div><div><h2 class="title" id="_important_files"><span class="number">8.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Important files</span> <a title="Permalink" class="permalink" href="#_important_files">#</a></h2></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.9.4.2.1"><span class="term ">/etc/salt/minion</span></dt><dd><p>Salt minion configuration file</p></dd><dt id="id-1.9.4.2.2"><span class="term ">/etc/salt/minion_id</span></dt><dd><p>Salt minion name. Useful if changed host name and need to change minion name accordingly.</p></dd><dt id="id-1.9.4.2.3"><span class="term ">/srv/pillar/ceph/deepsea_minions.sls</span></dt><dd><p>Deepsea minion targets</p></dd><dt id="id-1.9.4.2.4"><span class="term ">/srv/pillar/ceph/stack/ceph/cluster.yml</span></dt><dd><p>Deepsea cluster configuration for the cluster "ceph" (the default cluster name). After modification Deepsee stage 2 need to be run to make it in effect.</p></dd></dl></div><div class="variablelist "><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="name">Cluster configuration files: </span><a title="Permalink" class="permalink" href="#id-1.9.4.3">#</a></h6></div><dl class="variablelist"><dt id="id-1.9.4.3.2"><span class="term ">/srv/pillar/ceph/stack/global.yml</span></dt><dd><p>Affects all minions in the Salt cluster.</p></dd><dt id="id-1.9.4.3.3"><span class="term ">/srv/pillar/ceph/stack/ceph/cluster.yml</span></dt><dd><p>Affects all minions in the cluster named "ceph".</p></dd><dt id="id-1.9.4.3.4"><span class="term ">/srv/pillar/ceph/stack/ceph/roles/role.yml</span></dt><dd><p>Affects all minions that are assigned the specific role in the ceph cluster.</p></dd><dt id="id-1.9.4.3.5"><span class="term ">/srv/pillar/ceph/stack/cephminions/&lt;minion ID&gt;/yml</span></dt><dd><p>Affects the individual minion.</p></dd></dl></div></div><div class="sect1" id="_how_to_completely_uninstall_the_cluster_for_reinstall"><div class="titlepage"><div><div><h2 class="title" id="_how_to_completely_uninstall_the_cluster_for_reinstall"><span class="number">8.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How to completely uninstall the cluster for reinstall</span> <a title="Permalink" class="permalink" href="#_how_to_completely_uninstall_the_cluster_for_reinstall">#</a></h2></div></div></div><p>In case you did something wrong and would like to start over without re-installing the whole OS.</p><div class="verbatim-wrap"><pre class="screen"># salt-run disengage.safety
# salt-run state.orch ceph.purge</pre></div></div><div class="sect1" id="_how_to_get_salt_pillar_information"><div class="titlepage"><div><div><h2 class="title" id="_how_to_get_salt_pillar_information"><span class="number">8.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">How to get salt pillar information</span> <a title="Permalink" class="permalink" href="#_how_to_get_salt_pillar_information">#</a></h2></div></div></div><div class="verbatim-wrap"><pre class="screen"># salt '*' saltutil.pillar_refresh
# salt '*' pillar.items</pre></div><p>This will only give information after running stage 1 AKA the discovery stage.</p></div><div class="sect1" id="_ses_built_in_network_benchmark"><div class="titlepage"><div><div><h2 class="title" id="_ses_built_in_network_benchmark"><span class="number">8.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SES built-in network benchmark</span> <a title="Permalink" class="permalink" href="#_ses_built_in_network_benchmark">#</a></h2></div></div></div><p>See the <a class="link" href="https://www.suse.com/documentation/suse-enterprise-storage-5/singlehtml/book_storage_admin/book_storage_admin.html#storage.bp.performance.net_issues" target="_blank">Administration Guide</a></p><div class="verbatim-wrap"><pre class="screen"># salt-run net.iperf cluster=ceph output=full</pre></div></div><div class="sect1" id="_ceph_built_in_osd_benchmark"><div class="titlepage"><div><div><h2 class="title" id="_ceph_built_in_osd_benchmark"><span class="number">8.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ceph built-in OSD benchmark</span> <a title="Permalink" class="permalink" href="#_ceph_built_in_osd_benchmark">#</a></h2></div></div></div><p>See <a class="link" href="https://www.suse.com/documentation/suse-enterprise-storage-5/singlehtml/book_storage_admin/book_storage_admin.html#storage.bp.performance.slowosd" target="_blank">Administration Guide</a></p><div class="verbatim-wrap"><pre class="screen"># ceph tell osd.&lt;id&gt; bench</pre></div></div><div class="sect1" id="_ceph_built_in_pool_scope_benchmark"><div class="titlepage"><div><div><h2 class="title" id="_ceph_built_in_pool_scope_benchmark"><span class="number">8.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Ceph built-in pool scope benchmark</span> <a title="Permalink" class="permalink" href="#_ceph_built_in_pool_scope_benchmark">#</a></h2></div></div></div><div class="verbatim-wrap"><pre class="screen"># rados -p &lt;pool name&gt; bench 60 write</pre></div></div><div class="sect1" id="_interface_bonding"><div class="titlepage"><div><div><h2 class="title" id="_interface_bonding"><span class="number">8.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Interface bonding</span> <a title="Permalink" class="permalink" href="#_interface_bonding">#</a></h2></div></div></div><p>Use following parameters for the bonding module in 802.3ad mode (need switch support).</p><div class="verbatim-wrap"><pre class="screen">mode=802.3ad miimon=100 lacp_rate=fast xmit_hash_policy=layer3+4</pre></div></div><div class="sect1" id="_recommended_size_for_the_bluestores_wal_and_db_device"><div class="titlepage"><div><div><h2 class="title" id="_recommended_size_for_the_bluestores_wal_and_db_device"><span class="number">8.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Recommended Size for the BlueStore’s WAL and DB Device</span> <a title="Permalink" class="permalink" href="#_recommended_size_for_the_bluestores_wal_and_db_device">#</a></h2></div></div></div><p>See the <a class="link" href="https://documentation.suse.com/en-us/ses/6/single-html/ses-deployment/#rec-waldb-size" target="_blank">Deployment Guide</a></p></div><div class="sect1" id="_offline_setup"><div class="titlepage"><div><div><h2 class="title" id="_offline_setup"><span class="number">8.11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Offline setup</span> <a title="Permalink" class="permalink" href="#_offline_setup">#</a></h2></div></div></div><p>Setup a SMT or RMT server, and mirror below repositories from SCC.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>SLE-Product-SLES15-SP1-Pool</p></li><li class="listitem "><p>SLE-Product-SLES15-SP1-Updates</p></li><li class="listitem "><p>SLE-Module-Server-Applications15-SP1-Pool</p></li><li class="listitem "><p>SLE-Module-Server-Applications15-SP1-Updates</p></li><li class="listitem "><p>SLE-Module-Basesystem15-SP1-Pool</p></li><li class="listitem "><p>SLE-Module-Basesystem15-SP1-Updates</p></li><li class="listitem "><p>SUSE-Enterprise-Storage-6-Pool</p></li><li class="listitem "><p>SUSE-Enterprise-Storage-6-Updates</p></li></ul></div><p>Then point all nodes to the SMT/RMT server.</p></div><div class="sect1" id="_change_node_roles"><div class="titlepage"><div><div><h2 class="title" id="_change_node_roles"><span class="number">8.12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Change node roles</span> <a title="Permalink" class="permalink" href="#_change_node_roles">#</a></h2></div></div></div><p>After change of node roles by editing policy.cfg, need to run Stage 2 Configure to refresh configuration files.</p><div class="verbatim-wrap"><pre class="screen"># deepsea stage run ceph.stage.2</pre></div></div><div class="sect1" id="_more_tips"><div class="titlepage"><div><div><h2 class="title" id="_more_tips"><span class="number">8.13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">More tips</span> <a title="Permalink" class="permalink" href="#_more_tips">#</a></h2></div></div></div><p>Check the <a class="link" href="https://documentation.suse.com/ses/6/single-html/ses-admin/#part-troubleshooting" target="_blank">SES 6 Administration Guide</a> for more hints &amp; tips, FAQ, and troubleshooting techniques.</p></div></div><div class="chapter " id="_conclusion"><div class="titlepage"><div><div><h1 class="title"><span class="number">9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Conclusion</span> <a title="Permalink" class="permalink" href="#_conclusion">#</a></h1></div></div></div><div class="line"></div><p>The Huawei Taishan series represents a strong capacity-oriented platform. When combined with the access flexibility and reliability of SUSE Enterprise Storage and the industry leading support from Huawei, any business can feel confident in the ability to address the exponential growth in storage they are currently faced with.</p></div><div class="chapter " id="_references_and_resources"><div class="titlepage"><div><div><h1 class="title"><span class="number">10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">References and Resources</span> <a title="Permalink" class="permalink" href="#_references_and_resources">#</a></h1></div></div></div><div class="line"></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.11.2.1"><span class="term ">SUSE Enterprise Storage Technical Overview</span></dt><dd><p><a class="link" href="https://www.suse.com/media/white-paper/suse_enterprise_storage_technical_overview_wp.pdf" target="_blank">https://www.suse.com/media/white-paper/suse_enterprise_storage_technical_overview_wp.pdf</a></p></dd><dt id="id-1.11.2.2"><span class="term ">SUSE Enterprise Storage Tech Specs</span></dt><dd><p><a class="link" href="https://www.suse.com/products/suse-enterprise-storage/#tech-specs" target="_blank">https://www.suse.com/products/suse-enterprise-storage/#tech-specs</a></p></dd><dt id="id-1.11.2.3"><span class="term ">SUSE Enterprise Storage 6 - Release Notes</span></dt><dd><p><a class="link" href="https://www.suse.com/releasenotes/x86_64/SUSE-Enterprise-Storage/6/" target="_blank">https://www.suse.com/releasenotes/x86_64/SUSE-Enterprise-Storage/6/</a></p></dd><dt id="id-1.11.2.4"><span class="term ">SUSE Enterprise Storage 6 - Deployment Guide</span></dt><dd><p><a class="link" href="https://documentation.suse.com/ses/6/single-html/ses-deployment/#book-storage-deployment" target="_blank">https://documentation.suse.com/ses/6/single-html/ses-deployment/#book-storage-deployment</a></p></dd><dt id="id-1.11.2.5"><span class="term ">SUSE Enterprise Storage 6 - Administration Guide</span></dt><dd><p><a class="link" href="https://documentation.suse.com/ses/6/single-html/ses-admin/#book-storage-admin" target="_blank">https://documentation.suse.com/ses/6/single-html/ses-admin/#book-storage-admin</a></p></dd><dt id="id-1.11.2.6"><span class="term ">SUSE Linux Enterprise Server 15 SP1 - Deployment Guide</span></dt><dd><p><a class="link" href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-deployment/#book-sle-deployment" target="_blank">https://documentation.suse.com/sles/15-SP1/single-html/SLES-deployment/#book-sle-deployment</a></p></dd><dt id="id-1.11.2.7"><span class="term ">SUSE Linux Enterprise Server 15 SP1 - Administration Guide</span></dt><dd><p><a class="link" href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-admin/#book-sle-admin" target="_blank">https://documentation.suse.com/sles/15-SP1/single-html/SLES-admin/#book-sle-admin</a></p></dd><dt id="id-1.11.2.8"><span class="term ">SUSE Linux Enterprise Server 15 SP1 - Storage Administration Guide</span></dt><dd><p><a class="link" href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-storage/#book-storage" target="_blank">https://documentation.suse.com/sles/15-SP1/single-html/SLES-storage/#book-storage</a></p></dd><dt id="id-1.11.2.9"><span class="term ">SUSE Linux Enterprise Server 15 SP1 - Repository Mirroring Tool Guide</span></dt><dd><p><a class="link" href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-rmt/#book-rmt" target="_blank">https://documentation.suse.com/sles/15-SP1/single-html/SLES-rmt/#book-rmt</a></p></dd></dl></div></div><div class="appendix " id="appendix-policy-cfg"><div class="titlepage"><div><div><h1 class="title"><span class="number">A </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">policy.cfg example</span> <a title="Permalink" class="permalink" href="#appendix-policy-cfg">#</a></h1></div></div></div><div class="line"></div><div class="verbatim-wrap highlight yaml"><pre class="screen">## Cluster Assignment
cluster-ceph/cluster/*.sls

## Roles
# ADMIN
role-master/cluster/admin*.sls
role-admin/cluster/admin*.sls

# Monitoring
role-prometheus/cluster/admin*.sls
role-grafana/cluster/admin*.sls

# MON
role-mon/cluster/ceph[123]*.sls

# MGR (mgrs are usually colocated with mons)
role-mgr/cluster/ceph[123]*.sls

# MDS
role-mds/cluster/ceph2*.sls

# IGW
role-igw/cluster/ceph3*.sls

# RGW
role-rgw/cluster/ceph4*.sls

# NFS
# role-ganesha/cluster/ganesha*.sls

# COMMON
config/stack/default/global.yml
config/stack/default/ceph/cluster.yml

# Storage
role-storage/cluster/ceph[1234]*.sls</pre></div></div><div class="appendix " id="appendix-drive-groups-yml"><div class="titlepage"><div><div><h1 class="title"><span class="number">B </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">drive_groups.yml example</span> <a title="Permalink" class="permalink" href="#appendix-drive-groups-yml">#</a></h1></div></div></div><div class="line"></div><div class="verbatim-wrap highlight yaml"><pre class="screen">default:
  target: 'I@roles:storage'
  data_devices:
    # Use all hard disks as data device
    rotational: 1
  db_devices:
    # Use solid state drives as db device
    rotational: 0</pre></div></div></div></div><div class="page-bottom"><div id="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span id="_share-fb" class="bottom-button">Facebook</span><span class="spacer"> • </span><span id="_share-gp" class="bottom-button">Google+</span><span class="spacer"> • </span><span id="_share-tw" class="bottom-button">Twitter</span><span class="spacer"> • </span><span id="_share-mail" class="bottom-button">E-Mail</span></span></div><div class="print"><span id="_print-button" class="bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2020 
        SUSE</p><ul><li><a href="http://www.suse.com/company/careers/" target="_top">Careers</a></li><li><a href="http://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="http://www.suse.com/company/" target="_top">About</a></li><li><a href="http://www.suse.com/ContactsOffices/contacts_offices.jsp" target="_top">Contact Us</a></li></ul></div></div></body></html>