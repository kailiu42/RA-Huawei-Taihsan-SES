== Tips
=== Use a different NTP server
The default time server is the admin node. To change it, add
----
time_server: <server address>
----
in _/srv/pillar/ceph/stack/ceph/cluster.yml_

=== Copy files to all cluster nodes
`salt-cp` command can be used to copy files from the salt master node to minion nodes. This can be very convenient, for example, to keep /etc/hosts file in sync on all nodes.
----
salt-cp '*' /etc/hosts /etc/hosts
----

=== Important files
/etc/salt/minion::
Salt minion configuration file

/etc/salt/minion_id::
Salt minion name. Useful if changed host name and need to change minion name accordingly.

/srv/pillar/ceph/deepsea_minions.sls::
Deepsea minion targets

/srv/pillar/ceph/stack/ceph/cluster.yml::
Deepsea cluster configuration for the cluster "ceph" (the default cluster name). After modification Deepsee stage 2 need to be run to make it in effect.

.Cluster configuration files:
/srv/pillar/ceph/stack/global.yml::
Affects all minions in the Salt cluster.

/srv/pillar/ceph/stack/ceph/cluster.yml::
Affects all minions in the cluster named "ceph".

/srv/pillar/ceph/stack/ceph/roles/role.yml::
Affects all minions that are assigned the specific role in the ceph cluster.

/srv/pillar/ceph/stack/cephminions/<minion ID>/yml::
Affects the individual minion. 

=== How to completely uninstall the cluster for reinstall
In case you did something wrong and would like to start over without re-installing the whole OS.
----
# salt-run disengage.safety
# salt-run state.orch ceph.purge
----

=== How to get salt pillar information
----
# salt '*' saltutil.pillar_refresh
# salt '*' pillar.items
----
This will only give information after running stage 1 AKA the discovery stage.

=== SES built-in network benchmark
See the https://www.suse.com/documentation/suse-enterprise-storage-5/singlehtml/book_storage_admin/book_storage_admin.html#storage.bp.performance.net_issues[Administration Guide]

----
# salt-run net.iperf cluster=ceph output=full
----

=== Ceph built-in OSD benchmark
See https://www.suse.com/documentation/suse-enterprise-storage-5/singlehtml/book_storage_admin/book_storage_admin.html#storage.bp.performance.slowosd[Administration Guide]

----
# ceph tell osd.<id> bench
----

=== Ceph built-in pool scope benchmark
----
# rados -p <pool name> bench 60 write
----

=== Interface bonding
Use following parameters for the bonding module in 802.3ad mode (need switch support).
----
mode=802.3ad miimon=100 lacp_rate=fast xmit_hash_policy=layer3+4
----

=== Recommended Size for the BlueStore's WAL and DB Device
See the https://documentation.suse.com/en-us/ses/6/single-html/ses-deployment/#rec-waldb-size[Deployment Guide]


=== Offline setup
Setup a SMT or RMT server, and mirror below repositories from SCC.

* SLE-Product-SLES15-SP1-Pool
* SLE-Product-SLES15-SP1-Updates
* SLE-Module-Server-Applications15-SP1-Pool
* SLE-Module-Server-Applications15-SP1-Updates
* SLE-Module-Basesystem15-SP1-Pool
* SLE-Module-Basesystem15-SP1-Updates
* SUSE-Enterprise-Storage-6-Pool
* SUSE-Enterprise-Storage-6-Updates

Then point all nodes to the SMT/RMT server.

=== Change node roles
After change of node roles by editing policy.cfg, need to run Stage 2 Configure to refresh configuration files.
----
# deepsea stage run ceph.stage.2
----

=== More tips
Check the https://documentation.suse.com/ses/6/single-html/ses-admin/#part-troubleshooting[SES 6 Administration Guide] for more hints & tips, FAQ, and troubleshooting techniques.