ifeval::["{lang}" == "en"]
== Architectural Overview
This architecture overview section complements the https://www.suse.com/docrep/documents/1mdg7eq2kz/suse_enterprise_storage_technical_overview_wp.pdf[SUSE Enterprise Storage Technical Overview] document available online which presents the concepts behind software defined storage and Ceph as well as a quick start guide (non-platform specific).

=== Solution Architecture
SUSE Enterprise Storage provides unified block, file, and object access based on Ceph. Ceph is a distributed storage solution designed for scalability, reliability and performance. A critical component of Ceph is the RADOS object storage. RADOS enables a number of storage nodes to function together to store and retrieve data from the cluster using object storage techniques. The result is a storage solution that is abstracted from the hardware.

Ceph supports both native and traditional client access. The native clients are aware of the storage topology and communicate directly with the storage daemons over the public network, resulting in horizontally scaling performance. Non-native protocols, such as ISCSI, S3, and NFS require the use of gateways. While these gateways may be thought of as a limiting factor, the iSCSI and S3 gateways can scale horizontally using load balancing techniques.

[[img-ses-arch]]
.SES Architecture
image::SES-Reference-Architecture.png[SES Architecture,align=center,pdfwidth=100%,scaledwidth=100%]

In addition to the required network infrastructure, the minimum SUSE Enterprise Storage cluster comprises of a minimum of one administration server (physical or virtual), four object storage device nodes (OSDs), and three monitor nodes (MONs).

.Specific to this implementation:
 * One system is deployed as the administrative host server. The administration host is the Salt master and hosts the SUSE Enterprise Storage Administration Interface, openATTIC, which is the central management system which supports the cluster.
 * Three systems are deployed as monitor (MONs) nodes. Monitor nodes maintain information about the cluster health state, a map of the other monitor nodes and a CRUSH map. They also keep history of changes performed to the cluster.
 * It is strongly recommended to deploy monitors and other services on dedicated nodes. However due to shortage of equipment the monitors were deployed on the OSD nodes in this specific reference setup.
 * Additional servers may be deployed as iSCSI gateway nodes. iSCSI is a storage area network (SAN) protocol that allows clients (called initiators) to send SCSI command to SCSI storage devices (targets) on remote servers. This protocol is utilized for block-based connectivity to environments such as Microsoft Windows, VMware, and traditional UNIX. These systems may be scaled horizontally through client usage of multi-path technology.
 * The RADOS gateway provides S3 and Swift based access methods to the cluster. These nodes are generally situated behind a load balancer infrastructure to provide redundancy and scalability. It is important to note that the load generated by the RADOS gateway can consume a significant amount of compute and memory resources making the minimum recommended configuration contain 6-8 CPU cores and 32GB of RAM.
 * SUSE Enterprise Storage requires a minimum of four systems as storage nodes. The storage nodes contain individual storage devices that are each assigned an Object Storage Daemon (OSD). The OSD daemon assigned to the device stores data and manages the data replication and rebalancing processes. OSD daemons also communicate with the monitor (MON) nodes and provide them with the state of the other OSD daemons.

=== Networking Architecture
A software-defined solution is only as reliable as its slowest and least redundant component. This makes it important to design and implement a robust, high performance storage network infrastructure. From a network perspective for Ceph, this translates into:

* Separation of cluster (backend) and client-facing (public)network traffic. This isolates Ceph OSD daemon replication activities from Ceph clients. This may be achieved through separate physical networks or through use of VLANs.
* Redundancy and capacity in the form of bonded network interfaces connected to switches.

<<img-ceph-network>> shows the logical layout of the traditional Ceph cluster implementation.

[[img-ceph-network]]
.Ceph Network Architecture
image::Ceph-Network.png[Ceph Network Architecture,align=center,pdfwidth=100%,scaledwidth=100%]

=== Network/IP Address Scheme
Specific to this implementation, the following naming and addressing scheme were utilized.

.Node Roles and Network Addresses
[options=header,frame=topbot,grid=rows]
|===
|Role |Hostname |Public Network |Cluster Network
|Admin |sesadmin.example.com |10.1.1.3 |N/A
|Monitor |osd1.example.com |10.1.1.4 |N/A
|Monitor |osd2.example.com |10.1.1.5 |N/A
|Monitor |osd3.example.com |10.1.1.6 |N/A
|OSD Node |osd1.example.com |10.1.1.4 |10.2.1.4
|OSD Node |osd2.example.com |10.1.1.5 |10.2.1.5
|OSD Node |osd3.example.com |10.1.1.6 |10.2.1.6
|OSD Node |osd4.example.com |10.1.1.7 |10.2.1.7
|===
endif::[]
ifeval::["{lang}" == "zh_CN"]
endif::[]