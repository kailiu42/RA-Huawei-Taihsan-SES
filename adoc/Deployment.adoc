== Deployment
This deployment section should be seen as a supplement online https://www.suse.com/documentation/[documentation]. Specifically, the https://www.suse.com/documentation/suse-enterprise-storage-5/book_storage_deployment/data/book_storage_deployment.html[SUSE Enterprise Storage 5 Deployment Guide] as well as https://www.suse.com/documentation/sles-12/book_sle_admin/data/book_sle_admin.html[SUSE Linux Enterprise Server Administration Guide]. It is assumed that a Subscription Management Tool server exists within the environment. If not, please follow the information in https://www.suse.com/documentation/sles-12/book_smt/data/book_smt.html[Subscription Management Tool (SMT) for SLES] to make one available. The emphasis is on specific design and configuration choices.

=== Network Deployment Overview
The following considerations for the network configuration should be attended to:

* Ensure that all network switches are updated with consistent firmware versions.
ifeval::["{BondingType}" == "lacp"]
* Configure 802.3ad for system port bonding between the switches, plus enable jumbo frames.
* Specific configuration for this deployment can be found in Appendix C: Network Switch Configuration
// FIXME Appendix
endif::[]
* Network IP addressing and IP ranges need proper planning. In optimal environments, a single storage subnet should be used for all SUSE Enterprise Storage nodes on the primary network, with a separate, single subnet for the cluster network. Depending on the size of the installation, ranges larger than /24 may be required. When planning the network, current as well as future growth should be taken into consideration.
* Setup DNS A records for all nodes. Decide on subnets and VLANs and configure the switch ports accordingly.
* Ensure that you have access to a valid, reliable NTP service, as this is a critical requirement for all nodes. If not, it is recommended to use the admin node.

=== Hardware Recommended Actions
The following considerations for the hardware platforms should be attended to:

* Ensure Boot Mode is set to UEFI for all the physical nodes that comprise the SUSE Enterprise Storage Cluster.
* Verify BIOS/uEFI level on the physical servers correspond to those on the SUSE YES certification for all the platforms.
* Configure the boot media as RAID-1
* Configure all data and journal devices as individual RAID-0

ifeval::[{MellanoxCX4Present} == 1]
==== Set ConnectX-4 VPI NICS to Ethernet Mode
This configuration includes Mellanox ConnectX-4 VPI Network Interface Cards. These cards often arrive in Infiniband mode and need to be set to Ethernet mode. The way to affect this change involves following the steps outlined in the http://www.mellanox.com/pdf/prod_software/SUSE_Linux_Enterprise_Server_(SLES)_12_SP3_Driver_User_Manual.pdf[Mellanox manual for the inbox driver on SUSE Linux Enterprise 12 SP3].

_Replace the bold string with your PCI ID's_

The steps required are:
[subs="attributes,quotes"]
----
# zypper in mstflint
# lspci |grep Mellanox
# mstconfig -d **_Your_PCI_ID_** s LINK_TYPE_P1=ETH
# mstconfig -d **_Your_PCI_ID_** s LINK_TYPE_P2=ETH
# reboot
----
[[img-NICmode]]
.Change Mellanox ConnectX-4 VPI NIC Mode
image::ConnectX-4_NIC_mode.png[NIC Mode Change,align=center]
endif::[]

=== Operating System Installation
There are several key tasks to ensure are performed correctly during the operating system installation. During the SUSE Linux Enterprise installation, be sure and register the system with an update server. Ideally, this is a local SMT server which will reduce the time required for updates to be downloaded and applied to all nodes. By updating the nodes during installation, the system will deploy with the most up-to-date packages available, helping to ensure the best experience possible.

To speed installation, on the System Role screen, it is suggested to select KVM Virtualization Host. When the Installation Settings screen is reached, select *Software* and then un-check KVM Host Server. The resulting installation is a text mode server that is an appropriate base OS for SUSE Enterprise Server.

The next item is to ensure that the operating system is installed on the correct device. Especially on OSD nodes, the system may not choose the right drive by default. The proper way to ensure the right device is being used is to select *Create Partition Setup* on the Suggested Partitioning screen. This will then display a list of devices, allowing selection of the correct boot device. Next select *Edit Proposal Settings* and unselect the *Propose Separate Home Partition* checkbox.

Do ensure that NTP is configured to point to a valid, physical NTP server. This is critical for SUSE Enterprise Storage to function properly, and failure to do so can result in an unhealthy or non-functional cluster.

=== SUSE Enterprise Storage Installation & Configuration
==== Software Deployment configuration (Deepsea and Salt)
Salt, along with DeepSea, is a stack of components that help deploy and manage server infrastructure. It is very scalable, fast, and relatively easy to get running.

There are three key Salt imperatives that need to be followed:

* The Salt Master is the host that controls the entire cluster deployment. Ceph itself should NOT be running on the master as all resources should be dedicated to Salt master services. In our scenario, we used the Admin host as the Salt master.
* Salt minions are nodes controlled by Salt master. OSD, monitor, and gateway nodes are all Salt minions in this installation.
* Salt minions need to correctly resolve the Salt master’s host name over the network. This can be achieved through configuring unique host names per interface (eg osd1-cluster.suse.lab and osd1-public.suse.lab) in DNS and/or local /etc/hosts files.

Deepsea consists of series of Salt files to automate the deployment and management of a Ceph cluster. It consolidates the administrator’s decision making in a single location around cluster assignment, role assignment and profile assignment. Deepsea collects each set of tasks into a goal or stage.

The following steps, performed in order, will be used for this reference implementation:

. Install DeepSea on the Salt master which is the Admin node:
+
----
zypper in deepsea
----
+
. Start the salt-master service and enable:
+
----
systemctl start salt-master.service
systemctl enable salt-master.service
----
+
. Install the salt-minion on all cluster nodes (including the Admin):
+
----
zypper in salt-minion
----
+
. Configure all minions to connect to the Salt master:
+
Modify the entry for master in the _/etc/salt/minion_
+
----
master: sesadmin.domain.com
----
+
.	Start the salt-minion service and enable:
+
----
systemctl start salt-minion.service
systemctl enable salt-minion.service
----
+
.	List and accept all Salt keys on the Salt master: salt-key --accept-all and verify their acceptance
+
----
salt-key --list-all
salt-key –-accept-all
----
+
.	If the OSD nodes were used in a prior installation, zap ALL the OSD disks (ceph-disk zap <DISK>)
+
.	At this point, you can deploy and configure the cluster:
..	Prepare the cluster:
+
----
salt-run state.orch ceph.stage.prep
----
+
..	Run the discover stage to collect data from all minions and create configuration fragments:
+
----
salt-run state.orch ceph.stage.disovery
----
+
..	A proposal for the storage layout needs to be generated at this time. For the hardware configuration used for this work, the following command was utilized:
+
----
salt-run proposal.populate name=apollo ratio=9 wal=740-770 db=740-770 target='osd*' db-size=40g wal-size=5g data=5000-7000
----
+

+
The result of the above command is a deployment proposal for the disks that uses devices with a reported size of five to six terabytes for data and establishes a 5GB write-ahead log partition and a 40GB database partition for each spinner on one of the SSDs in a ratio of 9 spinners to each SSD.


..	A _/srv/pillar/ceph/proposals/policy.cfg_ file needs to be created to instruct Salt on the location and configuration files to use for the different components that make up the Ceph cluster (Salt master, admin, monitor, and OSDs).
***	See Appendix B for the _policy.cfg_ file used in the installation.

..	Next, proceed with the configuration stage to parse the _policy.cfg_ file and merge the included files into the final form
+
----
salt-run state.orch ceph.stage.configure
----
+
..	The last two steps manage the actual deployment.
+
Deploy monitors and ODS daemons first:
+
----
salt-run state.orch ceph.stage.deploy
----
+
NOTE: The command can take some time to complete, depending on the size of the cluster.
+
..	Check for successful completion via:
+
----
ceph –s
----
+
..	Finally, deploy the services-gateways (iSCSI, RADOS, and openATTIC to name a few):
+
----
salt-run state.orch ceph.stage.services
----

=== Post-deployment quick test
The steps below can be used (regardless of the deployment method) to validate the overall cluster health:

----
ceph status
ceph osd pool create test 1024
rados bench –p test 300 write --no-cleanup
rados bench –p test 300 seq
----

Once the tests are complete, you can remove the test pool via:

----
ceph tell mon.* injectargs --mon-allow-pool-delete=true
ceph osd pool delete test test --yes-i-really-really-mean-it
ceph tell mon.* injectargs --mon-allow-pool-delete=false
----

=== Deployment Considerations
Some final considerations before deploying your own version of a SUSE Enterprise Storage cluster, based on Ceph. As previously stated, please refer to the Administration and Deployment Guide.

* With the default replication setting of 3, remember that the client-facing network will have about half or less of the traffic of the backend network. This is especially true when component failures occur or rebalancing happens on the OSD nodes. For this reason, it is important not to under provision this critical cluster and service resource.
* It is important to maintain the minimum number of monitor nodes at three. As the cluster increases in size, it is best to increment in pairs, keeping the total number of Mon nodes as an odd number. However, only very large or very distributed clusters would likely need beyond the 3 MON nodes cited in this reference implementation. For performance reasons, it is recommended to use distinct nodes for the MON roles, so that the OSD nodes can be scaled as capacity requirements dictate.
* As described in this implementation guide and the SUSE Enterprise Storage documentation, a minimum of four OSD nodes is recommended, with the default replication setting of 3. This will ensure cluster operation, even with the loss of a complete OSD node. Generally speaking, performance of the overall cluster increases as more properly configured OSD nodes are added.