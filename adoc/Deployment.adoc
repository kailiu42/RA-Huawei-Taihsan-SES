ifeval::["{lang}" == "en"]
=== SUSE Enterprise Storage Installation & Configuration
==== Software Deployment Configuration (Deepsea and Salt)
Salt, along with DeepSea, is a stack of components that help deploy and manage server infrastructure. It is very scalable, fast, and relatively easy to get running.

There are three key Salt imperatives that need to be followed:

* The Salt Master is the host that controls the entire cluster deployment. Ceph itself should NOT be running on the master as all resources should be dedicated to Salt master services. In our scenario, we used the Admin host as the Salt master.
* Salt minions are nodes controlled by Salt master. OSD, monitor, and gateway nodes are all Salt minions in this installation.
* Salt minions need to correctly resolve the Salt master’s host name.

Deepsea consists of series of Salt files to automate the deployment and management of a Ceph cluster. It consolidates the administrator’s decision making in a single location around cluster layout, node role assignment and drive assignment. Deepsea collects each set of tasks into a goal or stage.

The following steps, performed in order, were used for this reference implementation. All commands were run by root user.

==== Prepare All Nodes
. Install salt master on the Admin node:
+
----
zypper in salt-master
----
+

. Start the salt-master service and enable start on boot:
+
----
systemctl enable --now salt-master.service
----
+

. Install the salt-minion on all cluster nodes (including the Admin):
+
----
zypper in salt-minion
----
+

. Configure all minions to connect to the Salt master:
+
Create a new file `/etc/salt/minion.d/master.conf` with the following content: 
+
----
master: admin.example.com
----
+

. Restart the salt-minion service and enable it:
+
----
systemctl restart salt-minion.service
systemctl enable salt-minion.service
----
+

. List Salt fingerprints on all the minions:
+
----
salt-call --local key.finger
----
+

. List all incoming minion fingerprints on the Salt master, verify them against the fingerprints on each minions to make sure they all match. If they do, accept all Salt keys on the Salt master:
+
----
salt-key -F
salt-key --accept-all
salt-key --list-all
----
+

. Verify if Salt works properly by "ping" each minions from the Salt master. They should all return True on success:
+
----
salt '*' test.ping
----

. Now check and make sure the time on all nodes are the same. In later stage Deepsea will setup all nodes to synchronize time from the admin node, but before that is done, strange errors may occur if time on each node are largely out of sync. So it's better to set all nodes to the same time manually first. For example, run below command on your admin node:
+
----
salt '*' cmd.run 'date -s "2020-03-19 17:30:00"'
----
Use your actual time in the same format when running the command. It doesn't have to be super accurate, as later all nodes will be synchronized by the chrony time service.


==== Prepare OSD Disks on OSD Nodes
If the OSD nodes were used in a prior installation, or the disks are used by other applications before, zap ALL the OSD disks first.

IMPORTANT: This must be done on all the OSD disks that were used before, or else the deployment may fail when activating OSDs.

WARNING: Below commands should not be copied and executed on your installation blindly. The device names used below are just examples, you need to change them to match only the OSD disks in your own installation. Failed to use the correct device name may erase your OS disk or other disks that may hold valuable data.

. Wipe the beginning of each partition:
+
----
for partition in /dev/sdX[0-9]*
do
  dd if=/dev/zero of=$partition bs=4096 count=1 oflag=direct
done
----
+

. Wipe the beginning of the drive:
+
----
dd if=/dev/zero of=/dev/sdX bs=512 count=34 oflag=direct
----
+
	
. Wipe the end of the drive:
+
----
dd if=/dev/zero of=/dev/sdX bs=512 count=33 \
  seek=$((`blockdev --getsz /dev/sdX` - 33)) oflag=direct
----

==== Install and Configure Deepsea on Admin Node
. Install deepsea package on Admin node:
+
----
# zypper in deepsea
----
+

. Check _/srv/pillar/ceph/master_minion.sls_ for correctness.

. Check _/srv/pillar/ceph/deepsea_minions.sls_ file, make sure the deepsea_minions option targets the correct nodes. In the usual case, it can simply be put like below to match all Salt minions in the cluster:
+
----
deepsea_minions: '*'
----
+

. Create _/srv/pillar/ceph/stack/ceph/cluster.yml_ [[create-cluster-yml,Create cluster.yml]] with below options: 
+
----
cluster_network: <net/mask of the cluster network>
public_network: <net/mask of the public network>
time_server: <Address of the NTP server, if this line is omitted admin node will be used>
----

==== Deploy Using Deepsea
At this point Deepsea commands can be run on the admin node to deploy the cluster.

[TIP]
====
Each command can be run either as:
----
salt-run state.orch ceph.stage.<stage name>
----

Or:
----
deepsea stage run ceph.stage.<stage name>
----

The latter form is preferred as it outputs real time progress.
====

===== Stage 0: Prepare
During this stage, all required updates are applied and your nodes may be rebooted.
----
deepsea stage run ceph.stage.0
----

IMPORTANT: If the Salt master reboots during Stage 0, you need to run Stage 0 again after it boots up.

Optionally, create the _/var/lib/ceph_ btrfs subvolume:
----
salt-run state.orch ceph.migrate.subvolume
----

===== Stage 1: Discovery
During this stage, all hardware in your cluster is detected and necessary information are collected for the Ceph configuration.
----
deepsea stage run ceph.stage.1
----

NOTE: Configure cluster and public network in _/srv/pillar/ceph/stack/ceph/cluster.yml_ if not yet done as described in <<create-cluster-yml>>.

Now a _/srv/pillar/ceph/proposals/policy.cfg_ file needs to be created to instruct Deepsea on the location and configuration files to use for the different components that make up the Ceph cluster (Salt master, admin, monitor, OSD and other roles).

To do so, copy the example file to the right location then edit it to match your installation: 
----
cp /usr/share/doc/packages/deepsea/examples/policy.cfg-rolebased /srv/pillar/ceph/proposals/policy.cfg
----

TIP: See <<appendix-policy-cfg>> for the one used when installing the cluster described in this document.

===== Stage 2: Configure
During this stage necessary configuration data are prepared in particular format.
----
deepsea stage run ceph.stage.2
----

[TIP]
====
Use below command to check the attributes of each node:
----
salt '*' pillar.items
----
Ensure the public and cluster network attributes are the same as configured.
====

===== Define drive groups
DriveGroups information are defined in the file _/srv/salt/ceph/configuration/files/drive_groups.yml_. It specifies what drives should be used for data device, DB device, or WAL device, and other parameters for setting up the OSDs.

. First take a look of all the disks on all OSD nodes:
+
----
salt-run disks.details
----
+
It lists the vendor, model, size and type of the disks. Those information can be used to match a group of drives and assign them to different uses.

. Now define drive groups in the drive_groups.yml file.
+
TIP: See <<appendix-drive-groups-yml>> for the drive group definition used in this example cluster.
For complete information refers to the https://documentation.suse.com/en-us/ses/6/single-html/ses-deployment/#ds-drive-groups[Deployment Guide]

. After finished editing _drive_groups.yml_, run below commands to see the result definition. Exam it carefully and make sure it meets your expectation before moving on to next step.
+
----
salt-run disks.list
salt-run disks.report
----

===== Stage 3: Deploy
A basic Ceph cluster with mandatory Ceph services is created in this stage.
----
deepsea stage run ceph.stage.3
----

NOTE: It may take quite some time for above command to finish if your cluster is large, your have a lot of disks, or your Internet bandwidth is limited while you didn't register the nodes to local SMT server.

After the above command is finished successfully, check whether the cluster is up by running:
----
ceph -s
----


===== Stage 4: Services
Additional features of Ceph like iSCSI, Object Gateway and CephFS can be installed in this stage. Each is optional and up to your situation. 
----
deepsea stage run ceph.stage.4
----

After the above command finishes successfully, the SUSE Enterprise Storage cluster is considered fully deployed.

=== Post-deployment Quick Tests
The steps below can be used to validate the overall cluster health, by creating a `test` storage pool and run some write and read test on it.
----
ceph status
ceph osd pool create test 1024
rados -p test bench 300 write --no-cleanup
rados -p test bench 300 seq
----

Once the tests are complete, you can remove the test pool via:
----
ceph tell mon.* injectargs --mon-allow-pool-delete=true
ceph osd pool delete test test --yes-i-really-really-mean-it
ceph tell mon.* injectargs --mon-allow-pool-delete=false
----

endif::[]
ifeval::["{lang}" == "zh_CN"]
endif::[]